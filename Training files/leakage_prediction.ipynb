{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "leakage prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXIWpEnEeIZj",
        "outputId": "66282fe9-2e45-4d3f-a635-fb1076618132"
      },
      "source": [
        "#Install non-standard packages (assuming jupyter notebook)\n",
        "!pip install shap\n",
        "!pip install lime\n",
        "!pip install eli5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.7/dist-packages (0.39.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.7/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (54.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.7/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: eli5 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from eli5) (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from eli5) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.22.2.post1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (1.19.5)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (20.3.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->eli5) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "3CuPhVQO3E0u",
        "outputId": "6089d7f9-7747-4d4a-ba3e-5467026955eb"
      },
      "source": [
        "# load the dataset \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "data = pd.read_excel('/content/drive/MyDrive/Pump Predictive Maintenance/Condition Hydraulic Pump/condition Hydraulic.xlsx')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>cycle_id</th>\n",
              "      <th>date</th>\n",
              "      <th>PS1</th>\n",
              "      <th>PS2</th>\n",
              "      <th>PS3</th>\n",
              "      <th>PS4</th>\n",
              "      <th>PS5</th>\n",
              "      <th>PS6</th>\n",
              "      <th>FS1</th>\n",
              "      <th>FS2</th>\n",
              "      <th>TS1</th>\n",
              "      <th>TS2</th>\n",
              "      <th>TS3</th>\n",
              "      <th>TS4</th>\n",
              "      <th>P1</th>\n",
              "      <th>VS1</th>\n",
              "      <th>CE1</th>\n",
              "      <th>CP1</th>\n",
              "      <th>SE1</th>\n",
              "      <th>cooler</th>\n",
              "      <th>valve</th>\n",
              "      <th>leakage</th>\n",
              "      <th>accumulator</th>\n",
              "      <th>stable</th>\n",
              "      <th>rul</th>\n",
              "      <th>label1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2019-01-01 00:00:00</td>\n",
              "      <td>160.673492</td>\n",
              "      <td>109.466914</td>\n",
              "      <td>1.991475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.842170</td>\n",
              "      <td>9.728097</td>\n",
              "      <td>6.709815</td>\n",
              "      <td>10.304592</td>\n",
              "      <td>35.621983</td>\n",
              "      <td>40.978767</td>\n",
              "      <td>38.471017</td>\n",
              "      <td>31.745250</td>\n",
              "      <td>2538.929167</td>\n",
              "      <td>0.576950</td>\n",
              "      <td>39.601350</td>\n",
              "      <td>1.862750</td>\n",
              "      <td>59.157183</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>35.166667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2019-01-01 00:10:00</td>\n",
              "      <td>160.603320</td>\n",
              "      <td>109.354890</td>\n",
              "      <td>1.976234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.635142</td>\n",
              "      <td>9.529488</td>\n",
              "      <td>6.715315</td>\n",
              "      <td>10.403098</td>\n",
              "      <td>36.676967</td>\n",
              "      <td>41.532767</td>\n",
              "      <td>38.978967</td>\n",
              "      <td>34.493867</td>\n",
              "      <td>2531.498900</td>\n",
              "      <td>0.565850</td>\n",
              "      <td>25.786433</td>\n",
              "      <td>1.255550</td>\n",
              "      <td>59.335617</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-01-01 00:20:00</td>\n",
              "      <td>160.347720</td>\n",
              "      <td>109.158845</td>\n",
              "      <td>1.972224</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.530548</td>\n",
              "      <td>9.427949</td>\n",
              "      <td>6.718522</td>\n",
              "      <td>10.366250</td>\n",
              "      <td>37.880800</td>\n",
              "      <td>42.442450</td>\n",
              "      <td>39.631950</td>\n",
              "      <td>35.646150</td>\n",
              "      <td>2519.928000</td>\n",
              "      <td>0.576533</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.113217</td>\n",
              "      <td>59.543150</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>34.833333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2019-01-01 00:30:00</td>\n",
              "      <td>160.188088</td>\n",
              "      <td>109.064807</td>\n",
              "      <td>1.946575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.438827</td>\n",
              "      <td>9.337430</td>\n",
              "      <td>6.720565</td>\n",
              "      <td>10.302678</td>\n",
              "      <td>38.879050</td>\n",
              "      <td>43.403983</td>\n",
              "      <td>40.403383</td>\n",
              "      <td>36.579467</td>\n",
              "      <td>2511.541633</td>\n",
              "      <td>0.569267</td>\n",
              "      <td>20.459817</td>\n",
              "      <td>1.062150</td>\n",
              "      <td>59.794900</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>34.666667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2019-01-01 00:40:00</td>\n",
              "      <td>160.000472</td>\n",
              "      <td>108.931434</td>\n",
              "      <td>1.922707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.358762</td>\n",
              "      <td>9.260636</td>\n",
              "      <td>6.690308</td>\n",
              "      <td>10.237750</td>\n",
              "      <td>39.803917</td>\n",
              "      <td>44.332750</td>\n",
              "      <td>41.310550</td>\n",
              "      <td>37.427900</td>\n",
              "      <td>2503.449500</td>\n",
              "      <td>0.577367</td>\n",
              "      <td>19.787017</td>\n",
              "      <td>1.070467</td>\n",
              "      <td>59.455267</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>34.500000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  cycle_id                date  ...  stable        rul  label1\n",
              "0           0         1 2019-01-01 00:00:00  ...       1  35.166667       0\n",
              "1           1         2 2019-01-01 00:10:00  ...       1  35.000000       0\n",
              "2           2         3 2019-01-01 00:20:00  ...       1  34.833333       0\n",
              "3           3         4 2019-01-01 00:30:00  ...       1  34.666667       0\n",
              "4           4         5 2019-01-01 00:40:00  ...       1  34.500000       0\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ueGcGSV3em1"
      },
      "source": [
        "df = pd.DataFrame(data, columns=['PS1','PS3', 'PS4', 'PS5', 'FS1', 'FS2', 'TS1', 'P1', 'VS1', 'CE1', 'CP1', 'SE1', 'leakage'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "icj3maSW3iXy",
        "outputId": "ea22b870-b36f-4ff7-f6ef-4db83b194eb4"
      },
      "source": [
        "df = df.fillna(method='ffill')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PS1</th>\n",
              "      <th>PS3</th>\n",
              "      <th>PS4</th>\n",
              "      <th>PS5</th>\n",
              "      <th>FS1</th>\n",
              "      <th>FS2</th>\n",
              "      <th>TS1</th>\n",
              "      <th>P1</th>\n",
              "      <th>VS1</th>\n",
              "      <th>CE1</th>\n",
              "      <th>CP1</th>\n",
              "      <th>SE1</th>\n",
              "      <th>leakage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>160.673492</td>\n",
              "      <td>1.991475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.842170</td>\n",
              "      <td>6.709815</td>\n",
              "      <td>10.304592</td>\n",
              "      <td>35.621983</td>\n",
              "      <td>2538.929167</td>\n",
              "      <td>0.576950</td>\n",
              "      <td>39.601350</td>\n",
              "      <td>1.862750</td>\n",
              "      <td>59.157183</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>160.603320</td>\n",
              "      <td>1.976234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.635142</td>\n",
              "      <td>6.715315</td>\n",
              "      <td>10.403098</td>\n",
              "      <td>36.676967</td>\n",
              "      <td>2531.498900</td>\n",
              "      <td>0.565850</td>\n",
              "      <td>25.786433</td>\n",
              "      <td>1.255550</td>\n",
              "      <td>59.335617</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>160.347720</td>\n",
              "      <td>1.972224</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.530548</td>\n",
              "      <td>6.718522</td>\n",
              "      <td>10.366250</td>\n",
              "      <td>37.880800</td>\n",
              "      <td>2519.928000</td>\n",
              "      <td>0.576533</td>\n",
              "      <td>25.786433</td>\n",
              "      <td>1.113217</td>\n",
              "      <td>59.543150</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>160.188088</td>\n",
              "      <td>1.946575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.438827</td>\n",
              "      <td>6.720565</td>\n",
              "      <td>10.302678</td>\n",
              "      <td>38.879050</td>\n",
              "      <td>2511.541633</td>\n",
              "      <td>0.569267</td>\n",
              "      <td>20.459817</td>\n",
              "      <td>1.062150</td>\n",
              "      <td>59.794900</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>160.000472</td>\n",
              "      <td>1.922707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.358762</td>\n",
              "      <td>6.690308</td>\n",
              "      <td>10.237750</td>\n",
              "      <td>39.803917</td>\n",
              "      <td>2503.449500</td>\n",
              "      <td>0.577367</td>\n",
              "      <td>19.787017</td>\n",
              "      <td>1.070467</td>\n",
              "      <td>59.455267</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          PS1       PS3  PS4       PS5  ...        CE1       CP1        SE1  leakage\n",
              "0  160.673492  1.991475  0.0  9.842170  ...  39.601350  1.862750  59.157183        0\n",
              "1  160.603320  1.976234  0.0  9.635142  ...  25.786433  1.255550  59.335617        0\n",
              "2  160.347720  1.972224  0.0  9.530548  ...  25.786433  1.113217  59.543150        0\n",
              "3  160.188088  1.946575  0.0  9.438827  ...  20.459817  1.062150  59.794900        0\n",
              "4  160.000472  1.922707  0.0  9.358762  ...  19.787017  1.070467  59.455267        0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP1n3QFK3kO7",
        "outputId": "8aef4214-9054-4336-a7f0-9e605c80b425"
      },
      "source": [
        "df['leakage'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUKKoDHs3n7c"
      },
      "source": [
        "X = df.iloc[:, :12]\n",
        "y = df.iloc[:, 12]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIlLx5N432z7"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "#test_y = le.fit_transform(test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZyJzM8R349N"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr4tzWfc37Y8"
      },
      "source": [
        "import keras\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes = 3)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmKn7rJ83-7r"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2eYHwrm4A17"
      },
      "source": [
        "# Part 2 - Now let's make the ANN!\n",
        "# import necessary modules  \n",
        "import pandas  as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "# Importing the Keras libraries and packages\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8lL6lpo4CzS"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "model = Sequential()\n",
        "model.add(Dense(12, activation='relu', input_dim=12))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "sgd = SGD(lr = 0.001, decay = 1e-6, momentum = 0.9, nesterov=True)\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iujGrXxh4GLB",
        "outputId": "3a4ff911-5f1b-4501-c717-a0df5db3e792"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs = 300, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "56/56 [==============================] - 1s 1ms/step - loss: 1.3375 - accuracy: 0.2322\n",
            "Epoch 2/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 1.0360 - accuracy: 0.5642\n",
            "Epoch 3/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.9923 - accuracy: 0.6221\n",
            "Epoch 4/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.9580 - accuracy: 0.6227\n",
            "Epoch 5/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.9465 - accuracy: 0.6043\n",
            "Epoch 6/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.9133 - accuracy: 0.6275\n",
            "Epoch 7/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.9058 - accuracy: 0.6204\n",
            "Epoch 8/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.8897 - accuracy: 0.6326\n",
            "Epoch 9/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8810 - accuracy: 0.6204\n",
            "Epoch 10/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.8671 - accuracy: 0.6233\n",
            "Epoch 11/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.8555 - accuracy: 0.6190\n",
            "Epoch 12/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.8427 - accuracy: 0.6205\n",
            "Epoch 13/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.8345 - accuracy: 0.6124\n",
            "Epoch 14/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.8076 - accuracy: 0.6146\n",
            "Epoch 15/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.7937 - accuracy: 0.6130\n",
            "Epoch 16/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.7801 - accuracy: 0.6142\n",
            "Epoch 17/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.7630 - accuracy: 0.6353\n",
            "Epoch 18/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.7453 - accuracy: 0.6712\n",
            "Epoch 19/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.7354 - accuracy: 0.6849\n",
            "Epoch 20/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.7306 - accuracy: 0.7027\n",
            "Epoch 21/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.6790 - accuracy: 0.7008\n",
            "Epoch 22/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.6827 - accuracy: 0.7158\n",
            "Epoch 23/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.6460 - accuracy: 0.7234\n",
            "Epoch 24/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.6263 - accuracy: 0.7547\n",
            "Epoch 25/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.5923 - accuracy: 0.7796\n",
            "Epoch 26/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.5872 - accuracy: 0.7700\n",
            "Epoch 27/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.7557\n",
            "Epoch 28/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.5190 - accuracy: 0.7967\n",
            "Epoch 29/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.5081 - accuracy: 0.7841\n",
            "Epoch 30/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.7916\n",
            "Epoch 31/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.4700 - accuracy: 0.8049\n",
            "Epoch 32/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.4470 - accuracy: 0.8112\n",
            "Epoch 33/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.4424 - accuracy: 0.8076\n",
            "Epoch 34/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.7914\n",
            "Epoch 35/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.4149 - accuracy: 0.7926\n",
            "Epoch 36/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.4085 - accuracy: 0.7879\n",
            "Epoch 37/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.4043 - accuracy: 0.8165\n",
            "Epoch 38/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3853 - accuracy: 0.8193\n",
            "Epoch 39/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8353\n",
            "Epoch 40/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3631 - accuracy: 0.8749\n",
            "Epoch 41/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8507\n",
            "Epoch 42/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3507 - accuracy: 0.8874\n",
            "Epoch 43/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8866\n",
            "Epoch 44/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3388 - accuracy: 0.8838\n",
            "Epoch 45/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3290 - accuracy: 0.8999\n",
            "Epoch 46/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3184 - accuracy: 0.9284\n",
            "Epoch 47/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.9226\n",
            "Epoch 48/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.9315\n",
            "Epoch 49/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.9154\n",
            "Epoch 50/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2917 - accuracy: 0.9302\n",
            "Epoch 51/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.9540\n",
            "Epoch 52/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.9506\n",
            "Epoch 53/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2817 - accuracy: 0.9484\n",
            "Epoch 54/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.9540\n",
            "Epoch 55/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2559 - accuracy: 0.9439\n",
            "Epoch 56/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2605 - accuracy: 0.9617\n",
            "Epoch 57/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2550 - accuracy: 0.9471\n",
            "Epoch 58/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.2436 - accuracy: 0.9498\n",
            "Epoch 59/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9589\n",
            "Epoch 60/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2350 - accuracy: 0.9792\n",
            "Epoch 61/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2322 - accuracy: 0.9496\n",
            "Epoch 62/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9703\n",
            "Epoch 63/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9749\n",
            "Epoch 64/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9726\n",
            "Epoch 65/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9646\n",
            "Epoch 66/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9678\n",
            "Epoch 67/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9724\n",
            "Epoch 68/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9716\n",
            "Epoch 69/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9758\n",
            "Epoch 70/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.1882 - accuracy: 0.9758\n",
            "Epoch 71/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9718\n",
            "Epoch 72/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9747\n",
            "Epoch 73/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9686\n",
            "Epoch 74/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9697\n",
            "Epoch 75/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9735\n",
            "Epoch 76/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9751\n",
            "Epoch 77/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9750\n",
            "Epoch 78/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9811\n",
            "Epoch 79/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9703\n",
            "Epoch 80/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9853\n",
            "Epoch 81/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9758\n",
            "Epoch 82/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9828\n",
            "Epoch 83/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9808\n",
            "Epoch 84/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9797\n",
            "Epoch 85/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9802\n",
            "Epoch 86/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9836\n",
            "Epoch 87/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9859\n",
            "Epoch 88/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9833\n",
            "Epoch 89/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9871\n",
            "Epoch 90/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9745\n",
            "Epoch 91/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9826\n",
            "Epoch 92/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9826\n",
            "Epoch 93/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9697\n",
            "Epoch 94/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9783\n",
            "Epoch 95/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9870\n",
            "Epoch 96/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9839\n",
            "Epoch 97/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9819\n",
            "Epoch 98/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9843\n",
            "Epoch 99/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9815\n",
            "Epoch 100/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9897\n",
            "Epoch 101/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9862\n",
            "Epoch 102/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9835\n",
            "Epoch 103/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9878\n",
            "Epoch 104/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9885\n",
            "Epoch 105/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9798\n",
            "Epoch 106/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9933\n",
            "Epoch 107/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9822\n",
            "Epoch 108/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9749\n",
            "Epoch 109/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9859\n",
            "Epoch 110/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9887\n",
            "Epoch 111/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9867\n",
            "Epoch 112/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.9840\n",
            "Epoch 113/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.9856\n",
            "Epoch 114/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.9883\n",
            "Epoch 115/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9863\n",
            "Epoch 116/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.9864\n",
            "Epoch 117/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.9857\n",
            "Epoch 118/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9858\n",
            "Epoch 119/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9866\n",
            "Epoch 120/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.9879\n",
            "Epoch 121/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0671 - accuracy: 0.9846\n",
            "Epoch 122/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9876\n",
            "Epoch 123/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0548 - accuracy: 0.9882\n",
            "Epoch 124/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9865\n",
            "Epoch 125/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9866\n",
            "Epoch 126/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9873\n",
            "Epoch 127/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0537 - accuracy: 0.9906\n",
            "Epoch 128/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9875\n",
            "Epoch 129/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9848\n",
            "Epoch 130/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9899\n",
            "Epoch 131/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9908\n",
            "Epoch 132/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9869\n",
            "Epoch 133/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9879\n",
            "Epoch 134/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9859\n",
            "Epoch 135/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0480 - accuracy: 0.9920\n",
            "Epoch 136/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9908\n",
            "Epoch 137/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9872\n",
            "Epoch 138/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9893\n",
            "Epoch 139/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 0.9879\n",
            "Epoch 140/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9900\n",
            "Epoch 141/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0465 - accuracy: 0.9894\n",
            "Epoch 142/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0462 - accuracy: 0.9893\n",
            "Epoch 143/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9918\n",
            "Epoch 144/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0450 - accuracy: 0.9907\n",
            "Epoch 145/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0456 - accuracy: 0.9902\n",
            "Epoch 146/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0439 - accuracy: 0.9883\n",
            "Epoch 147/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0469 - accuracy: 0.9867\n",
            "Epoch 148/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0428 - accuracy: 0.9885\n",
            "Epoch 149/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0437 - accuracy: 0.9924\n",
            "Epoch 150/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9842\n",
            "Epoch 151/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0450 - accuracy: 0.9829\n",
            "Epoch 152/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9902\n",
            "Epoch 153/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9895\n",
            "Epoch 154/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0417 - accuracy: 0.9920\n",
            "Epoch 155/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0458 - accuracy: 0.9860\n",
            "Epoch 156/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 0.9907\n",
            "Epoch 157/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0344 - accuracy: 0.9918\n",
            "Epoch 158/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0445 - accuracy: 0.9901\n",
            "Epoch 159/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0445 - accuracy: 0.9909\n",
            "Epoch 160/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9896\n",
            "Epoch 161/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0400 - accuracy: 0.9863\n",
            "Epoch 162/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0365 - accuracy: 0.9912\n",
            "Epoch 163/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9896\n",
            "Epoch 164/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0379 - accuracy: 0.9919\n",
            "Epoch 165/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9871\n",
            "Epoch 166/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9923\n",
            "Epoch 167/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0375 - accuracy: 0.9874\n",
            "Epoch 168/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0372 - accuracy: 0.9890\n",
            "Epoch 169/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9909\n",
            "Epoch 170/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0330 - accuracy: 0.9906\n",
            "Epoch 171/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9855\n",
            "Epoch 172/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0344 - accuracy: 0.9949\n",
            "Epoch 173/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0397 - accuracy: 0.9889\n",
            "Epoch 174/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9922\n",
            "Epoch 175/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0335 - accuracy: 0.9921\n",
            "Epoch 176/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9847\n",
            "Epoch 177/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0329 - accuracy: 0.9932\n",
            "Epoch 178/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0325 - accuracy: 0.9950\n",
            "Epoch 179/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0356 - accuracy: 0.9923\n",
            "Epoch 180/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.9916\n",
            "Epoch 181/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9902\n",
            "Epoch 182/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0307 - accuracy: 0.9951\n",
            "Epoch 183/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9884\n",
            "Epoch 184/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0321 - accuracy: 0.9923\n",
            "Epoch 185/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0327 - accuracy: 0.9925\n",
            "Epoch 186/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0331 - accuracy: 0.9917\n",
            "Epoch 187/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0326 - accuracy: 0.9918\n",
            "Epoch 188/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0305 - accuracy: 0.9928\n",
            "Epoch 189/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0306 - accuracy: 0.9888\n",
            "Epoch 190/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0285 - accuracy: 0.9954\n",
            "Epoch 191/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 0.9891\n",
            "Epoch 192/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0277 - accuracy: 0.9954\n",
            "Epoch 193/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9894\n",
            "Epoch 194/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.9931\n",
            "Epoch 195/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0314 - accuracy: 0.9943\n",
            "Epoch 196/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0322 - accuracy: 0.9899\n",
            "Epoch 197/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9928\n",
            "Epoch 198/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0326 - accuracy: 0.9919\n",
            "Epoch 199/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0331 - accuracy: 0.9906\n",
            "Epoch 200/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.9919\n",
            "Epoch 201/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0335 - accuracy: 0.9920\n",
            "Epoch 202/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0299 - accuracy: 0.9926\n",
            "Epoch 203/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0334 - accuracy: 0.9870\n",
            "Epoch 204/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0316 - accuracy: 0.9933\n",
            "Epoch 205/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0330 - accuracy: 0.9894\n",
            "Epoch 206/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0321 - accuracy: 0.9919\n",
            "Epoch 207/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9937\n",
            "Epoch 208/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 0.9907\n",
            "Epoch 209/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0281 - accuracy: 0.9918\n",
            "Epoch 210/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.9894\n",
            "Epoch 211/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.9922\n",
            "Epoch 212/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0306 - accuracy: 0.9904\n",
            "Epoch 213/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9936\n",
            "Epoch 214/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0277 - accuracy: 0.9914\n",
            "Epoch 215/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0305 - accuracy: 0.9907\n",
            "Epoch 216/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9902\n",
            "Epoch 217/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0283 - accuracy: 0.9938\n",
            "Epoch 218/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0284 - accuracy: 0.9911\n",
            "Epoch 219/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0232 - accuracy: 0.9963\n",
            "Epoch 220/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.9925\n",
            "Epoch 221/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9961\n",
            "Epoch 222/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0263 - accuracy: 0.9927\n",
            "Epoch 223/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.9971\n",
            "Epoch 224/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0246 - accuracy: 0.9949\n",
            "Epoch 225/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0266 - accuracy: 0.9923\n",
            "Epoch 226/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0268 - accuracy: 0.9954\n",
            "Epoch 227/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9953\n",
            "Epoch 228/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0297 - accuracy: 0.9887\n",
            "Epoch 229/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9916\n",
            "Epoch 230/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 0.9920\n",
            "Epoch 231/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.9965\n",
            "Epoch 232/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0288 - accuracy: 0.9901\n",
            "Epoch 233/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9905\n",
            "Epoch 234/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9957\n",
            "Epoch 235/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9955\n",
            "Epoch 236/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9943\n",
            "Epoch 237/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 0.9920\n",
            "Epoch 238/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9941\n",
            "Epoch 239/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9936\n",
            "Epoch 240/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9944\n",
            "Epoch 241/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9972\n",
            "Epoch 242/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0256 - accuracy: 0.9896\n",
            "Epoch 243/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0241 - accuracy: 0.9946\n",
            "Epoch 244/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.9945\n",
            "Epoch 245/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9965\n",
            "Epoch 246/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9970\n",
            "Epoch 247/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0385 - accuracy: 0.9843\n",
            "Epoch 248/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9980\n",
            "Epoch 249/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9947\n",
            "Epoch 250/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.9925\n",
            "Epoch 251/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9922\n",
            "Epoch 252/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9959\n",
            "Epoch 253/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9954\n",
            "Epoch 254/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9946\n",
            "Epoch 255/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9921\n",
            "Epoch 256/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0228 - accuracy: 0.9945\n",
            "Epoch 257/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.9954\n",
            "Epoch 258/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0225 - accuracy: 0.9932\n",
            "Epoch 259/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0234 - accuracy: 0.9938\n",
            "Epoch 260/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.9916\n",
            "Epoch 261/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9950\n",
            "Epoch 262/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 0.9943\n",
            "Epoch 263/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9961\n",
            "Epoch 264/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.9964\n",
            "Epoch 265/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 0.9984\n",
            "Epoch 266/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.9966\n",
            "Epoch 267/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.9950\n",
            "Epoch 268/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0184 - accuracy: 0.9963\n",
            "Epoch 269/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0194 - accuracy: 0.9943\n",
            "Epoch 270/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9962\n",
            "Epoch 271/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9955\n",
            "Epoch 272/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9947\n",
            "Epoch 273/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9936\n",
            "Epoch 274/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0227 - accuracy: 0.9936\n",
            "Epoch 275/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9949\n",
            "Epoch 276/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9915\n",
            "Epoch 277/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0209 - accuracy: 0.9965\n",
            "Epoch 278/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.9898\n",
            "Epoch 279/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9941\n",
            "Epoch 280/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0317 - accuracy: 0.9888\n",
            "Epoch 281/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9976\n",
            "Epoch 282/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9920\n",
            "Epoch 283/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.9945\n",
            "Epoch 284/300\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.9943\n",
            "Epoch 285/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9909\n",
            "Epoch 286/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0208 - accuracy: 0.9925\n",
            "Epoch 287/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 0.9952\n",
            "Epoch 288/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.9970\n",
            "Epoch 289/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0149 - accuracy: 0.9970\n",
            "Epoch 290/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0214 - accuracy: 0.9944\n",
            "Epoch 291/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0193 - accuracy: 0.9956\n",
            "Epoch 292/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9969\n",
            "Epoch 293/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 0.9965\n",
            "Epoch 294/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0160 - accuracy: 0.9956\n",
            "Epoch 295/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0171 - accuracy: 0.9974\n",
            "Epoch 296/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 0.9953\n",
            "Epoch 297/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0179 - accuracy: 0.9956\n",
            "Epoch 298/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9943\n",
            "Epoch 299/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9971\n",
            "Epoch 300/300\n",
            "56/56 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.9956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWYSBUG04ISo",
        "outputId": "5a93e3fd-c3e0-42b7-f40b-ad529cc64c7f"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose = 0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.023345986381173134\n",
            "Test accuracy: 0.9909297227859497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSdZAWv84Oei",
        "outputId": "3f0df1a7-0ec6-424d-a12a-7102012f46f0"
      },
      "source": [
        "y_test_class = np.argmax(y_test, axis=1)\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_test_class, y_pred_class))\n",
        "print(confusion_matrix(y_test_class, y_pred_class))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       228\n",
            "           1       0.99      0.97      0.98       101\n",
            "           2       0.97      0.99      0.98       112\n",
            "\n",
            "    accuracy                           0.99       441\n",
            "   macro avg       0.99      0.99      0.99       441\n",
            "weighted avg       0.99      0.99      0.99       441\n",
            "\n",
            "[[228   0   0]\n",
            " [  0  98   3]\n",
            " [  0   1 111]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEZL_faqWtbl"
      },
      "source": [
        "model.save('leakage.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "A91q-c1psBH3",
        "outputId": "f65734ce-aaa5-4508-d90d-8403d628c512"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import f1_score\n",
        "# perform permutation importance\n",
        "results = permutation_importance(model, X_train, y_train, scoring='f1')\n",
        "# get importance\n",
        "importance = results.importances_mean\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
        "# plot feature importance\n",
        "pyplot.bar([x for x in range(len(importance))], importance)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a163e94aad2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# perform permutation importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# get importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimportance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportances_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/inspection/_permutation_importance.py\u001b[0m in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mbaseline_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     scores = Parallel(n_jobs=n_jobs)(delayed(_calculate_permutation_scores)(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    167\u001b[0m                           stacklevel=2)\n\u001b[1;32m    168\u001b[0m         return self._score(partial(_cached_call, None), estimator, X, y_true,\n\u001b[0;32m--> 169\u001b[0;31m                            sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_factory_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[0;32m--> 212\u001b[0;31m                                                  **self._kwargs)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "qc9hzQ9HBqD3",
        "outputId": "95dfe72a-e866-4f1d-d434-d3a4d47c973e"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PS1</th>\n",
              "      <th>PS3</th>\n",
              "      <th>PS4</th>\n",
              "      <th>PS5</th>\n",
              "      <th>FS1</th>\n",
              "      <th>FS2</th>\n",
              "      <th>TS1</th>\n",
              "      <th>P1</th>\n",
              "      <th>VS1</th>\n",
              "      <th>CE1</th>\n",
              "      <th>CP1</th>\n",
              "      <th>SE1</th>\n",
              "      <th>leakage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1743</th>\n",
              "      <td>160.682812</td>\n",
              "      <td>1.966124</td>\n",
              "      <td>10.111747</td>\n",
              "      <td>9.878805</td>\n",
              "      <td>6.523390</td>\n",
              "      <td>10.175785</td>\n",
              "      <td>35.972317</td>\n",
              "      <td>2550.211033</td>\n",
              "      <td>0.544350</td>\n",
              "      <td>47.611933</td>\n",
              "      <td>2.168950</td>\n",
              "      <td>57.215783</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1948</th>\n",
              "      <td>160.492123</td>\n",
              "      <td>1.910307</td>\n",
              "      <td>10.085908</td>\n",
              "      <td>9.855468</td>\n",
              "      <td>6.372402</td>\n",
              "      <td>10.156538</td>\n",
              "      <td>36.199967</td>\n",
              "      <td>2565.915267</td>\n",
              "      <td>0.539217</td>\n",
              "      <td>46.897250</td>\n",
              "      <td>2.128733</td>\n",
              "      <td>54.709350</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1149</th>\n",
              "      <td>158.464413</td>\n",
              "      <td>1.762262</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.146661</td>\n",
              "      <td>6.470398</td>\n",
              "      <td>9.697162</td>\n",
              "      <td>44.578933</td>\n",
              "      <td>2471.423300</td>\n",
              "      <td>0.610183</td>\n",
              "      <td>27.758567</td>\n",
              "      <td>1.757550</td>\n",
              "      <td>57.702650</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>918</th>\n",
              "      <td>158.352453</td>\n",
              "      <td>1.782746</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.998992</td>\n",
              "      <td>6.637715</td>\n",
              "      <td>9.615803</td>\n",
              "      <td>46.438917</td>\n",
              "      <td>2442.769367</td>\n",
              "      <td>0.625183</td>\n",
              "      <td>26.461633</td>\n",
              "      <td>1.721600</td>\n",
              "      <td>59.820967</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1906</th>\n",
              "      <td>161.117303</td>\n",
              "      <td>1.993822</td>\n",
              "      <td>0.188128</td>\n",
              "      <td>9.846316</td>\n",
              "      <td>6.694112</td>\n",
              "      <td>10.168597</td>\n",
              "      <td>36.237150</td>\n",
              "      <td>2541.379400</td>\n",
              "      <td>0.534950</td>\n",
              "      <td>47.453417</td>\n",
              "      <td>2.173750</td>\n",
              "      <td>58.804550</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>158.938173</td>\n",
              "      <td>1.799205</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>9.146790</td>\n",
              "      <td>6.686517</td>\n",
              "      <td>9.705005</td>\n",
              "      <td>44.610567</td>\n",
              "      <td>2465.650067</td>\n",
              "      <td>0.603783</td>\n",
              "      <td>27.686700</td>\n",
              "      <td>1.781083</td>\n",
              "      <td>59.909350</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>157.886552</td>\n",
              "      <td>1.727004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.030977</td>\n",
              "      <td>6.372348</td>\n",
              "      <td>9.629887</td>\n",
              "      <td>45.961133</td>\n",
              "      <td>2471.505767</td>\n",
              "      <td>0.624350</td>\n",
              "      <td>27.078950</td>\n",
              "      <td>1.712250</td>\n",
              "      <td>56.917817</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638</th>\n",
              "      <td>172.801707</td>\n",
              "      <td>1.104562</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.408293</td>\n",
              "      <td>3.266573</td>\n",
              "      <td>8.911047</td>\n",
              "      <td>57.118100</td>\n",
              "      <td>2621.011100</td>\n",
              "      <td>0.732583</td>\n",
              "      <td>18.647217</td>\n",
              "      <td>1.468933</td>\n",
              "      <td>30.315450</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>156.147247</td>\n",
              "      <td>1.641392</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.532672</td>\n",
              "      <td>6.276127</td>\n",
              "      <td>9.203810</td>\n",
              "      <td>54.197267</td>\n",
              "      <td>2415.724067</td>\n",
              "      <td>0.652667</td>\n",
              "      <td>20.244567</td>\n",
              "      <td>1.552733</td>\n",
              "      <td>56.681733</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1943</th>\n",
              "      <td>160.472172</td>\n",
              "      <td>1.912446</td>\n",
              "      <td>10.085205</td>\n",
              "      <td>9.854015</td>\n",
              "      <td>6.378222</td>\n",
              "      <td>10.154337</td>\n",
              "      <td>36.219100</td>\n",
              "      <td>2565.180633</td>\n",
              "      <td>0.538833</td>\n",
              "      <td>47.011700</td>\n",
              "      <td>2.135583</td>\n",
              "      <td>54.882750</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             PS1       PS3        PS4  ...       CP1        SE1  leakage\n",
              "1743  160.682812  1.966124  10.111747  ...  2.168950  57.215783        1\n",
              "1948  160.492123  1.910307  10.085908  ...  2.128733  54.709350        2\n",
              "1149  158.464413  1.762262   0.000000  ...  1.757550  57.702650        1\n",
              "918   158.352453  1.782746   0.000000  ...  1.721600  59.820967        0\n",
              "1906  161.117303  1.993822   0.188128  ...  2.173750  58.804550        0\n",
              "1179  158.938173  1.799205   0.000023  ...  1.781083  59.909350        0\n",
              "982   157.886552  1.727004   0.000000  ...  1.712250  56.917817        2\n",
              "638   172.801707  1.104562   0.000000  ...  1.468933  30.315450        2\n",
              "236   156.147247  1.641392   0.000000  ...  1.552733  56.681733        2\n",
              "1943  160.472172  1.912446  10.085205  ...  2.135583  54.882750        2\n",
              "\n",
              "[10 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTr91M_OOD3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a6a84e-82a4-41a6-d9c1-65544f18d952"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeClassifier    \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
        "from sklearn import tree\n",
        "\n",
        "clf_gini = DecisionTreeClassifier(criterion = \"entropy\", max_depth=3)\n",
        "\n",
        "clf_gini.fit(X_train, y_train)\n",
        "y_pred_gini = clf_gini.predict(X_test)  \n",
        "\n",
        "print (\"Accuracy : \", accuracy_score(y_test,y_pred_gini)*100)         # Evaulating predictions with test labels\n",
        "print (\"Report : \",  classification_report(y_test, y_pred_gini))\n",
        "text_representation = tree.export_text(clf_gini, feature_names=X.columns.tolist())\n",
        "print(text_representation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  91.60997732426304\n",
            "Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       228\n",
            "           1       0.76      0.93      0.84       101\n",
            "           2       0.92      0.74      0.82       112\n",
            "\n",
            "    accuracy                           0.92       441\n",
            "   macro avg       0.89      0.89      0.89       441\n",
            "weighted avg       0.92      0.92      0.92       441\n",
            "\n",
            "|--- SE1 <= 57.99\n",
            "|   |--- FS1 <= 6.44\n",
            "|   |   |--- TS1 <= 54.35\n",
            "|   |   |   |--- class: 2\n",
            "|   |   |--- TS1 >  54.35\n",
            "|   |   |   |--- class: 1\n",
            "|   |--- FS1 >  6.44\n",
            "|   |   |--- FS1 <= 6.60\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- FS1 >  6.60\n",
            "|   |   |   |--- class: 0\n",
            "|--- SE1 >  57.99\n",
            "|   |--- FS1 <= 6.50\n",
            "|   |   |--- class: 1\n",
            "|   |--- FS1 >  6.50\n",
            "|   |   |--- class: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEPAU8y9ZnWM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "cc503d6e-8ca7-4c5f-ef1e-d4cb54b7214c"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "# get importance\n",
        "importance = clf_gini.feature_importances_\n",
        "# summarize feature importance\n",
        "for feature_names,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (feature_names,v))\n",
        "# plot feature importance\n",
        "pyplot.bar([x for x in range(len(importance))], importance, tick_label= X.columns)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature: 0, Score: 0.00000\n",
            "Feature: 1, Score: 0.00000\n",
            "Feature: 2, Score: 0.00000\n",
            "Feature: 3, Score: 0.00000\n",
            "Feature: 4, Score: 0.16365\n",
            "Feature: 5, Score: 0.00000\n",
            "Feature: 6, Score: 0.06096\n",
            "Feature: 7, Score: 0.00000\n",
            "Feature: 8, Score: 0.00000\n",
            "Feature: 9, Score: 0.00000\n",
            "Feature: 10, Score: 0.00000\n",
            "Feature: 11, Score: 0.77539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLOj7YFEvRef"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}