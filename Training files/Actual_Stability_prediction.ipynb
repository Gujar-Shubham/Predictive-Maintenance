{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Actual Stability prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Si-7fvgB7L",
        "outputId": "dad96fdd-5e07-4cd6-a0df-22f40f5e69eb"
      },
      "source": [
        "#Install non-standard packages (assuming jupyter notebook)\n",
        "!pip install shap\n",
        "!pip install lime\n",
        "!pip install eli5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f4/c5b95cddae15be80f8e58b25edceca105aa83c0b8c86a1edad24a6af80d3/shap-0.39.0.tar.gz (356kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.41.1)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (54.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491626 sha256=9bb9af705185ffaf35f04eb083f97c34bc677ad53b889202271a5df2e5746b1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/27/f5/a8ab9da52fd159aae6477b5ede6eaaec69fd130fa0fa59f283\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.39.0 slicer-0.0.7\n",
            "Collecting lime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/86/91a13127d83d793ecb50eb75e716f76e6eda809b6803c5a4ff462339789e/lime-0.2.0.1.tar.gz (275kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-cp37-none-any.whl size=283846 sha256=6dc0130c96261afcd2053d0849ed20b4359e2cc27e5b39aa7837791669e94f38\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/4f/a5/0bc765457bd41378bf3ce8d17d7495369d6e7ca3b712c60c89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n",
            "Collecting eli5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/54/04cab6e1c0ae535bec93f795d8403fdf6caf66fa5a6512263202dbb14ea6/eli5-0.11.0-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.22.2.post1)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (20.3.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from eli5) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from eli5) (2.11.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->eli5) (1.1.1)\n",
            "Installing collected packages: eli5\n",
            "Successfully installed eli5-0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "CoDRXIMesCv8",
        "outputId": "0ee301a1-ac0f-43ac-8422-3fd364cf886d"
      },
      "source": [
        "# load the dataset \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "data = pd.read_excel('/content/drive/MyDrive/Pump Predictive Maintenance/Condition Hydraulic Pump/condition Hydraulic.xlsx')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>cycle_id</th>\n",
              "      <th>date</th>\n",
              "      <th>PS1</th>\n",
              "      <th>PS2</th>\n",
              "      <th>PS3</th>\n",
              "      <th>PS4</th>\n",
              "      <th>PS5</th>\n",
              "      <th>PS6</th>\n",
              "      <th>FS1</th>\n",
              "      <th>FS2</th>\n",
              "      <th>TS1</th>\n",
              "      <th>TS2</th>\n",
              "      <th>TS3</th>\n",
              "      <th>TS4</th>\n",
              "      <th>P1</th>\n",
              "      <th>VS1</th>\n",
              "      <th>CE1</th>\n",
              "      <th>CP1</th>\n",
              "      <th>SE1</th>\n",
              "      <th>cooler</th>\n",
              "      <th>valve</th>\n",
              "      <th>leakage</th>\n",
              "      <th>accumulator</th>\n",
              "      <th>stable</th>\n",
              "      <th>rul</th>\n",
              "      <th>label1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2019-01-01 00:00:00</td>\n",
              "      <td>160.673492</td>\n",
              "      <td>109.466914</td>\n",
              "      <td>1.991475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.842170</td>\n",
              "      <td>9.728097</td>\n",
              "      <td>6.709815</td>\n",
              "      <td>10.304592</td>\n",
              "      <td>35.621983</td>\n",
              "      <td>40.978767</td>\n",
              "      <td>38.471017</td>\n",
              "      <td>31.745250</td>\n",
              "      <td>2538.929167</td>\n",
              "      <td>0.576950</td>\n",
              "      <td>39.601350</td>\n",
              "      <td>1.862750</td>\n",
              "      <td>59.157183</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>35.166667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2019-01-01 00:10:00</td>\n",
              "      <td>160.603320</td>\n",
              "      <td>109.354890</td>\n",
              "      <td>1.976234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.635142</td>\n",
              "      <td>9.529488</td>\n",
              "      <td>6.715315</td>\n",
              "      <td>10.403098</td>\n",
              "      <td>36.676967</td>\n",
              "      <td>41.532767</td>\n",
              "      <td>38.978967</td>\n",
              "      <td>34.493867</td>\n",
              "      <td>2531.498900</td>\n",
              "      <td>0.565850</td>\n",
              "      <td>25.786433</td>\n",
              "      <td>1.255550</td>\n",
              "      <td>59.335617</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-01-01 00:20:00</td>\n",
              "      <td>160.347720</td>\n",
              "      <td>109.158845</td>\n",
              "      <td>1.972224</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.530548</td>\n",
              "      <td>9.427949</td>\n",
              "      <td>6.718522</td>\n",
              "      <td>10.366250</td>\n",
              "      <td>37.880800</td>\n",
              "      <td>42.442450</td>\n",
              "      <td>39.631950</td>\n",
              "      <td>35.646150</td>\n",
              "      <td>2519.928000</td>\n",
              "      <td>0.576533</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.113217</td>\n",
              "      <td>59.543150</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>34.833333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2019-01-01 00:30:00</td>\n",
              "      <td>160.188088</td>\n",
              "      <td>109.064807</td>\n",
              "      <td>1.946575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.438827</td>\n",
              "      <td>9.337430</td>\n",
              "      <td>6.720565</td>\n",
              "      <td>10.302678</td>\n",
              "      <td>38.879050</td>\n",
              "      <td>43.403983</td>\n",
              "      <td>40.403383</td>\n",
              "      <td>36.579467</td>\n",
              "      <td>2511.541633</td>\n",
              "      <td>0.569267</td>\n",
              "      <td>20.459817</td>\n",
              "      <td>1.062150</td>\n",
              "      <td>59.794900</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>34.666667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2019-01-01 00:40:00</td>\n",
              "      <td>160.000472</td>\n",
              "      <td>108.931434</td>\n",
              "      <td>1.922707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.358762</td>\n",
              "      <td>9.260636</td>\n",
              "      <td>6.690308</td>\n",
              "      <td>10.237750</td>\n",
              "      <td>39.803917</td>\n",
              "      <td>44.332750</td>\n",
              "      <td>41.310550</td>\n",
              "      <td>37.427900</td>\n",
              "      <td>2503.449500</td>\n",
              "      <td>0.577367</td>\n",
              "      <td>19.787017</td>\n",
              "      <td>1.070467</td>\n",
              "      <td>59.455267</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "      <td>34.500000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  cycle_id                date  ...  stable        rul  label1\n",
              "0           0         1 2019-01-01 00:00:00  ...       1  35.166667       0\n",
              "1           1         2 2019-01-01 00:10:00  ...       1  35.000000       0\n",
              "2           2         3 2019-01-01 00:20:00  ...       1  34.833333       0\n",
              "3           3         4 2019-01-01 00:30:00  ...       1  34.666667       0\n",
              "4           4         5 2019-01-01 00:40:00  ...       1  34.500000       0\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwD9V6pQsb22",
        "outputId": "18e8c30f-baba-4a43-b399-501aac5f03f1"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0     0\n",
              "cycle_id       0\n",
              "date           0\n",
              "PS1            0\n",
              "PS2            0\n",
              "PS3            0\n",
              "PS4            0\n",
              "PS5            0\n",
              "PS6            0\n",
              "FS1            0\n",
              "FS2            0\n",
              "TS1            0\n",
              "TS2            0\n",
              "TS3            0\n",
              "TS4            0\n",
              "P1             0\n",
              "VS1            0\n",
              "CE1            1\n",
              "CP1            0\n",
              "SE1            0\n",
              "cooler         0\n",
              "valve          0\n",
              "leakage        0\n",
              "accumulator    0\n",
              "stable         0\n",
              "rul            0\n",
              "label1         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKUB7Aw2sier"
      },
      "source": [
        "df = pd.DataFrame(data, columns=['date','PS1','PS3', 'PS4', 'PS5', 'FS1', 'FS2', 'TS1', 'P1', 'VS1', 'CE1', 'CP1', 'SE1', 'cooler', 'valve', 'leakage', 'accumulator', 'label1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX2MRAzGs3mR"
      },
      "source": [
        "df = df.fillna(method='ffill')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "k3OVJG7Os-QK",
        "outputId": "96fbd5b4-4343-4e94-f54b-78b2898986e2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>PS1</th>\n",
              "      <th>PS3</th>\n",
              "      <th>PS4</th>\n",
              "      <th>PS5</th>\n",
              "      <th>FS1</th>\n",
              "      <th>FS2</th>\n",
              "      <th>TS1</th>\n",
              "      <th>P1</th>\n",
              "      <th>VS1</th>\n",
              "      <th>CE1</th>\n",
              "      <th>CP1</th>\n",
              "      <th>SE1</th>\n",
              "      <th>cooler</th>\n",
              "      <th>valve</th>\n",
              "      <th>leakage</th>\n",
              "      <th>accumulator</th>\n",
              "      <th>label1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-01-01 00:00:00</td>\n",
              "      <td>160.673492</td>\n",
              "      <td>1.991475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.842170</td>\n",
              "      <td>6.709815</td>\n",
              "      <td>10.304592</td>\n",
              "      <td>35.621983</td>\n",
              "      <td>2538.929167</td>\n",
              "      <td>0.576950</td>\n",
              "      <td>39.601350</td>\n",
              "      <td>1.862750</td>\n",
              "      <td>59.157183</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-01-01 00:10:00</td>\n",
              "      <td>160.603320</td>\n",
              "      <td>1.976234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.635142</td>\n",
              "      <td>6.715315</td>\n",
              "      <td>10.403098</td>\n",
              "      <td>36.676967</td>\n",
              "      <td>2531.498900</td>\n",
              "      <td>0.565850</td>\n",
              "      <td>25.786433</td>\n",
              "      <td>1.255550</td>\n",
              "      <td>59.335617</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-01 00:20:00</td>\n",
              "      <td>160.347720</td>\n",
              "      <td>1.972224</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.530548</td>\n",
              "      <td>6.718522</td>\n",
              "      <td>10.366250</td>\n",
              "      <td>37.880800</td>\n",
              "      <td>2519.928000</td>\n",
              "      <td>0.576533</td>\n",
              "      <td>25.786433</td>\n",
              "      <td>1.113217</td>\n",
              "      <td>59.543150</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-01-01 00:30:00</td>\n",
              "      <td>160.188088</td>\n",
              "      <td>1.946575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.438827</td>\n",
              "      <td>6.720565</td>\n",
              "      <td>10.302678</td>\n",
              "      <td>38.879050</td>\n",
              "      <td>2511.541633</td>\n",
              "      <td>0.569267</td>\n",
              "      <td>20.459817</td>\n",
              "      <td>1.062150</td>\n",
              "      <td>59.794900</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-01-01 00:40:00</td>\n",
              "      <td>160.000472</td>\n",
              "      <td>1.922707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.358762</td>\n",
              "      <td>6.690308</td>\n",
              "      <td>10.237750</td>\n",
              "      <td>39.803917</td>\n",
              "      <td>2503.449500</td>\n",
              "      <td>0.577367</td>\n",
              "      <td>19.787017</td>\n",
              "      <td>1.070467</td>\n",
              "      <td>59.455267</td>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 date         PS1       PS3  ...  leakage  accumulator  label1\n",
              "0 2019-01-01 00:00:00  160.673492  1.991475  ...        0          130       0\n",
              "1 2019-01-01 00:10:00  160.603320  1.976234  ...        0          130       0\n",
              "2 2019-01-01 00:20:00  160.347720  1.972224  ...        0          130       0\n",
              "3 2019-01-01 00:30:00  160.188088  1.946575  ...        0          130       0\n",
              "4 2019-01-01 00:40:00  160.000472  1.922707  ...        0          130       0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "P0u1B4Uf8N-d",
        "outputId": "a4728f49-06e4-4139-9ede-bd55f7e66766"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PS1</th>\n",
              "      <th>PS3</th>\n",
              "      <th>PS4</th>\n",
              "      <th>PS5</th>\n",
              "      <th>FS1</th>\n",
              "      <th>FS2</th>\n",
              "      <th>TS1</th>\n",
              "      <th>P1</th>\n",
              "      <th>VS1</th>\n",
              "      <th>CE1</th>\n",
              "      <th>CP1</th>\n",
              "      <th>SE1</th>\n",
              "      <th>cooler</th>\n",
              "      <th>valve</th>\n",
              "      <th>leakage</th>\n",
              "      <th>accumulator</th>\n",
              "      <th>label1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "      <td>2205.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>160.485315</td>\n",
              "      <td>1.753227</td>\n",
              "      <td>2.600266</td>\n",
              "      <td>9.163320</td>\n",
              "      <td>6.198549</td>\n",
              "      <td>9.649453</td>\n",
              "      <td>45.424567</td>\n",
              "      <td>2495.509203</td>\n",
              "      <td>0.613315</td>\n",
              "      <td>31.300695</td>\n",
              "      <td>1.808399</td>\n",
              "      <td>55.287900</td>\n",
              "      <td>41.240816</td>\n",
              "      <td>90.693878</td>\n",
              "      <td>0.669388</td>\n",
              "      <td>107.199546</td>\n",
              "      <td>0.700680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.699425</td>\n",
              "      <td>0.251902</td>\n",
              "      <td>4.279355</td>\n",
              "      <td>0.576296</td>\n",
              "      <td>1.032883</td>\n",
              "      <td>0.449246</td>\n",
              "      <td>7.991933</td>\n",
              "      <td>73.836682</td>\n",
              "      <td>0.060260</td>\n",
              "      <td>11.574310</td>\n",
              "      <td>0.278263</td>\n",
              "      <td>8.960189</td>\n",
              "      <td>42.383143</td>\n",
              "      <td>10.681802</td>\n",
              "      <td>0.817233</td>\n",
              "      <td>16.435848</td>\n",
              "      <td>0.458064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>155.391547</td>\n",
              "      <td>0.840252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.365800</td>\n",
              "      <td>2.018572</td>\n",
              "      <td>8.857513</td>\n",
              "      <td>35.313783</td>\n",
              "      <td>2361.747267</td>\n",
              "      <td>0.524367</td>\n",
              "      <td>17.555983</td>\n",
              "      <td>1.062150</td>\n",
              "      <td>18.276617</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>158.100195</td>\n",
              "      <td>1.729733</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.547239</td>\n",
              "      <td>6.391670</td>\n",
              "      <td>9.203397</td>\n",
              "      <td>36.237150</td>\n",
              "      <td>2442.933467</td>\n",
              "      <td>0.555100</td>\n",
              "      <td>20.084650</td>\n",
              "      <td>1.550100</td>\n",
              "      <td>56.270183</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>158.960895</td>\n",
              "      <td>1.779631</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.115781</td>\n",
              "      <td>6.576673</td>\n",
              "      <td>9.692270</td>\n",
              "      <td>44.836650</td>\n",
              "      <td>2480.926633</td>\n",
              "      <td>0.610183</td>\n",
              "      <td>27.392533</td>\n",
              "      <td>1.739683</td>\n",
              "      <td>58.758150</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>161.000735</td>\n",
              "      <td>1.932047</td>\n",
              "      <td>3.503266</td>\n",
              "      <td>9.844351</td>\n",
              "      <td>6.657508</td>\n",
              "      <td>10.155008</td>\n",
              "      <td>54.104317</td>\n",
              "      <td>2548.211467</td>\n",
              "      <td>0.649850</td>\n",
              "      <td>46.677383</td>\n",
              "      <td>2.148483</td>\n",
              "      <td>59.656900</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>180.922708</td>\n",
              "      <td>2.023398</td>\n",
              "      <td>10.207068</td>\n",
              "      <td>9.978510</td>\n",
              "      <td>6.722707</td>\n",
              "      <td>10.403098</td>\n",
              "      <td>57.899283</td>\n",
              "      <td>2740.641000</td>\n",
              "      <td>0.839067</td>\n",
              "      <td>47.903667</td>\n",
              "      <td>2.840100</td>\n",
              "      <td>60.755300</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               PS1          PS3  ...  accumulator       label1\n",
              "count  2205.000000  2205.000000  ...  2205.000000  2205.000000\n",
              "mean    160.485315     1.753227  ...   107.199546     0.700680\n",
              "std       4.699425     0.251902  ...    16.435848     0.458064\n",
              "min     155.391547     0.840252  ...    90.000000     0.000000\n",
              "25%     158.100195     1.729733  ...    90.000000     0.000000\n",
              "50%     158.960895     1.779631  ...   100.000000     1.000000\n",
              "75%     161.000735     1.932047  ...   130.000000     1.000000\n",
              "max     180.922708     2.023398  ...   130.000000     1.000000\n",
              "\n",
              "[8 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v2qjvvvtCE5"
      },
      "source": [
        "X = df.iloc[:,1:13]\n",
        "y = df.iloc[:, 17]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "52J3I5B1tLvj",
        "outputId": "218eb10b-a957-43d0-fa65-098ba06febcb"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PS1</th>\n",
              "      <th>PS3</th>\n",
              "      <th>PS4</th>\n",
              "      <th>PS5</th>\n",
              "      <th>FS1</th>\n",
              "      <th>FS2</th>\n",
              "      <th>TS1</th>\n",
              "      <th>P1</th>\n",
              "      <th>VS1</th>\n",
              "      <th>CE1</th>\n",
              "      <th>CP1</th>\n",
              "      <th>SE1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>160.673492</td>\n",
              "      <td>1.991475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.842170</td>\n",
              "      <td>6.709815</td>\n",
              "      <td>10.304592</td>\n",
              "      <td>35.621983</td>\n",
              "      <td>2538.929167</td>\n",
              "      <td>0.576950</td>\n",
              "      <td>39.601350</td>\n",
              "      <td>1.862750</td>\n",
              "      <td>59.157183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>160.603320</td>\n",
              "      <td>1.976234</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.635142</td>\n",
              "      <td>6.715315</td>\n",
              "      <td>10.403098</td>\n",
              "      <td>36.676967</td>\n",
              "      <td>2531.498900</td>\n",
              "      <td>0.565850</td>\n",
              "      <td>25.786433</td>\n",
              "      <td>1.255550</td>\n",
              "      <td>59.335617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>160.347720</td>\n",
              "      <td>1.972224</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.530548</td>\n",
              "      <td>6.718522</td>\n",
              "      <td>10.366250</td>\n",
              "      <td>37.880800</td>\n",
              "      <td>2519.928000</td>\n",
              "      <td>0.576533</td>\n",
              "      <td>25.786433</td>\n",
              "      <td>1.113217</td>\n",
              "      <td>59.543150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>160.188088</td>\n",
              "      <td>1.946575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.438827</td>\n",
              "      <td>6.720565</td>\n",
              "      <td>10.302678</td>\n",
              "      <td>38.879050</td>\n",
              "      <td>2511.541633</td>\n",
              "      <td>0.569267</td>\n",
              "      <td>20.459817</td>\n",
              "      <td>1.062150</td>\n",
              "      <td>59.794900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>160.000472</td>\n",
              "      <td>1.922707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.358762</td>\n",
              "      <td>6.690308</td>\n",
              "      <td>10.237750</td>\n",
              "      <td>39.803917</td>\n",
              "      <td>2503.449500</td>\n",
              "      <td>0.577367</td>\n",
              "      <td>19.787017</td>\n",
              "      <td>1.070467</td>\n",
              "      <td>59.455267</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          PS1       PS3  PS4  ...        CE1       CP1        SE1\n",
              "0  160.673492  1.991475  0.0  ...  39.601350  1.862750  59.157183\n",
              "1  160.603320  1.976234  0.0  ...  25.786433  1.255550  59.335617\n",
              "2  160.347720  1.972224  0.0  ...  25.786433  1.113217  59.543150\n",
              "3  160.188088  1.946575  0.0  ...  20.459817  1.062150  59.794900\n",
              "4  160.000472  1.922707  0.0  ...  19.787017  1.070467  59.455267\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp13cV9rtNLA",
        "outputId": "b12d75a0-6854-4e41-8b44-6e83214711e7"
      },
      "source": [
        "y.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "Name: label1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U9YkW6duYsM",
        "outputId": "a35d21d6-7ce8-4148-a326-ad738bedea23"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2205, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tsPMTW98k4G"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "#test_y = le.fit_transform(test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCrpPxshufGz"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96UtUpdY_yxX"
      },
      "source": [
        "import keras\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes = 2)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZolNhy-OuiVa"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc6JUyY_upuX",
        "outputId": "efdbc308-793c-4b55-efec-a3ee70b48620"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViyO-vkxusSF"
      },
      "source": [
        "# Part 2 - Now let's make the ANN!\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuPJqvgDutnt"
      },
      "source": [
        "# Initialising the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(units = 15,kernel_initializer='he_uniform',activation='relu',input_dim = 12))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 8, kernel_initializer = 'he_uniform',activation='relu'))\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 2, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XydO8Ib8u0Rv",
        "outputId": "84dd91e1-71fd-4d7d-fba9-79defb6089cf"
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 15)                195       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 128       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 18        \n",
            "=================================================================\n",
            "Total params: 341\n",
            "Trainable params: 341\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axMmwwmiu2XB",
        "outputId": "b8c1f22e-6ee1-4b86-b4c6-5dbd2dd5ccc2"
      },
      "source": [
        "model_history=classifier.fit(X_train, y_train, validation_split=0.2, batch_size = 16, epochs = 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "89/89 [==============================] - 4s 7ms/step - loss: 0.7356 - accuracy: 0.6701 - val_loss: 0.5956 - val_accuracy: 0.7422\n",
            "Epoch 2/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.5376 - accuracy: 0.7886 - val_loss: 0.5277 - val_accuracy: 0.7394\n",
            "Epoch 3/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.4677 - accuracy: 0.7901 - val_loss: 0.4893 - val_accuracy: 0.7734\n",
            "Epoch 4/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8391 - val_loss: 0.4543 - val_accuracy: 0.8527\n",
            "Epoch 5/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.8814 - val_loss: 0.4213 - val_accuracy: 0.8810\n",
            "Epoch 6/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.9051 - val_loss: 0.3877 - val_accuracy: 0.8980\n",
            "Epoch 7/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.9079 - val_loss: 0.3562 - val_accuracy: 0.9235\n",
            "Epoch 8/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.3001 - accuracy: 0.9194 - val_loss: 0.3277 - val_accuracy: 0.9320\n",
            "Epoch 9/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.9201 - val_loss: 0.3132 - val_accuracy: 0.9263\n",
            "Epoch 10/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.9246 - val_loss: 0.2919 - val_accuracy: 0.9377\n",
            "Epoch 11/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.9414 - val_loss: 0.2808 - val_accuracy: 0.9433\n",
            "Epoch 12/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.9324 - val_loss: 0.2714 - val_accuracy: 0.9320\n",
            "Epoch 13/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9375 - val_loss: 0.2612 - val_accuracy: 0.9490\n",
            "Epoch 14/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1993 - accuracy: 0.9432 - val_loss: 0.2537 - val_accuracy: 0.9490\n",
            "Epoch 15/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9562 - val_loss: 0.2471 - val_accuracy: 0.9547\n",
            "Epoch 16/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1787 - accuracy: 0.9533 - val_loss: 0.2399 - val_accuracy: 0.9547\n",
            "Epoch 17/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9483 - val_loss: 0.2400 - val_accuracy: 0.9490\n",
            "Epoch 18/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9337 - val_loss: 0.2328 - val_accuracy: 0.9490\n",
            "Epoch 19/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1830 - accuracy: 0.9371 - val_loss: 0.2312 - val_accuracy: 0.9575\n",
            "Epoch 20/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.9400 - val_loss: 0.2281 - val_accuracy: 0.9518\n",
            "Epoch 21/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9533 - val_loss: 0.2262 - val_accuracy: 0.9490\n",
            "Epoch 22/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1366 - accuracy: 0.9588 - val_loss: 0.2294 - val_accuracy: 0.9603\n",
            "Epoch 23/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1459 - accuracy: 0.9540 - val_loss: 0.2232 - val_accuracy: 0.9518\n",
            "Epoch 24/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1312 - accuracy: 0.9670 - val_loss: 0.2226 - val_accuracy: 0.9547\n",
            "Epoch 25/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9590 - val_loss: 0.2299 - val_accuracy: 0.9603\n",
            "Epoch 26/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1469 - accuracy: 0.9565 - val_loss: 0.2210 - val_accuracy: 0.9575\n",
            "Epoch 27/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9505 - val_loss: 0.2286 - val_accuracy: 0.9547\n",
            "Epoch 28/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.9513 - val_loss: 0.2288 - val_accuracy: 0.9632\n",
            "Epoch 29/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9669 - val_loss: 0.2229 - val_accuracy: 0.9518\n",
            "Epoch 30/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1274 - accuracy: 0.9584 - val_loss: 0.2200 - val_accuracy: 0.9575\n",
            "Epoch 31/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1478 - accuracy: 0.9510 - val_loss: 0.2222 - val_accuracy: 0.9575\n",
            "Epoch 32/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1457 - accuracy: 0.9556 - val_loss: 0.2208 - val_accuracy: 0.9575\n",
            "Epoch 33/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9619 - val_loss: 0.2200 - val_accuracy: 0.9490\n",
            "Epoch 34/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9526 - val_loss: 0.2180 - val_accuracy: 0.9547\n",
            "Epoch 35/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1276 - accuracy: 0.9581 - val_loss: 0.2286 - val_accuracy: 0.9575\n",
            "Epoch 36/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1125 - accuracy: 0.9706 - val_loss: 0.2207 - val_accuracy: 0.9547\n",
            "Epoch 37/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1145 - accuracy: 0.9699 - val_loss: 0.2170 - val_accuracy: 0.9575\n",
            "Epoch 38/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.9627 - val_loss: 0.2164 - val_accuracy: 0.9547\n",
            "Epoch 39/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1467 - accuracy: 0.9524 - val_loss: 0.2182 - val_accuracy: 0.9547\n",
            "Epoch 40/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1390 - accuracy: 0.9559 - val_loss: 0.2156 - val_accuracy: 0.9518\n",
            "Epoch 41/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9649 - val_loss: 0.2141 - val_accuracy: 0.9603\n",
            "Epoch 42/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1263 - accuracy: 0.9646 - val_loss: 0.2113 - val_accuracy: 0.9575\n",
            "Epoch 43/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9556 - val_loss: 0.2122 - val_accuracy: 0.9603\n",
            "Epoch 44/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9615 - val_loss: 0.2112 - val_accuracy: 0.9603\n",
            "Epoch 45/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1110 - accuracy: 0.9659 - val_loss: 0.2112 - val_accuracy: 0.9547\n",
            "Epoch 46/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1164 - accuracy: 0.9680 - val_loss: 0.2092 - val_accuracy: 0.9547\n",
            "Epoch 47/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.9547 - val_loss: 0.2098 - val_accuracy: 0.9632\n",
            "Epoch 48/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.9602 - val_loss: 0.2115 - val_accuracy: 0.9632\n",
            "Epoch 49/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9671 - val_loss: 0.2058 - val_accuracy: 0.9490\n",
            "Epoch 50/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9625 - val_loss: 0.2088 - val_accuracy: 0.9547\n",
            "Epoch 51/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9612 - val_loss: 0.2072 - val_accuracy: 0.9632\n",
            "Epoch 52/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1164 - accuracy: 0.9686 - val_loss: 0.2067 - val_accuracy: 0.9632\n",
            "Epoch 53/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1418 - accuracy: 0.9541 - val_loss: 0.2079 - val_accuracy: 0.9632\n",
            "Epoch 54/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1077 - accuracy: 0.9717 - val_loss: 0.2041 - val_accuracy: 0.9632\n",
            "Epoch 55/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1261 - accuracy: 0.9651 - val_loss: 0.2015 - val_accuracy: 0.9547\n",
            "Epoch 56/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.9585 - val_loss: 0.2039 - val_accuracy: 0.9547\n",
            "Epoch 57/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1340 - accuracy: 0.9514 - val_loss: 0.2063 - val_accuracy: 0.9632\n",
            "Epoch 58/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1349 - accuracy: 0.9592 - val_loss: 0.2016 - val_accuracy: 0.9575\n",
            "Epoch 59/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.9648 - val_loss: 0.1981 - val_accuracy: 0.9603\n",
            "Epoch 60/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9742 - val_loss: 0.1965 - val_accuracy: 0.9632\n",
            "Epoch 61/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9784 - val_loss: 0.2025 - val_accuracy: 0.9547\n",
            "Epoch 62/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1067 - accuracy: 0.9718 - val_loss: 0.1943 - val_accuracy: 0.9603\n",
            "Epoch 63/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1123 - accuracy: 0.9677 - val_loss: 0.2009 - val_accuracy: 0.9603\n",
            "Epoch 64/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1058 - accuracy: 0.9701 - val_loss: 0.1947 - val_accuracy: 0.9575\n",
            "Epoch 65/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9674 - val_loss: 0.2015 - val_accuracy: 0.9603\n",
            "Epoch 66/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9758 - val_loss: 0.1958 - val_accuracy: 0.9603\n",
            "Epoch 67/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9720 - val_loss: 0.1939 - val_accuracy: 0.9632\n",
            "Epoch 68/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.9620 - val_loss: 0.1938 - val_accuracy: 0.9632\n",
            "Epoch 69/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9668 - val_loss: 0.1906 - val_accuracy: 0.9603\n",
            "Epoch 70/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1178 - accuracy: 0.9651 - val_loss: 0.1907 - val_accuracy: 0.9632\n",
            "Epoch 71/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.9742 - val_loss: 0.1886 - val_accuracy: 0.9603\n",
            "Epoch 72/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9690 - val_loss: 0.1892 - val_accuracy: 0.9603\n",
            "Epoch 73/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.9714 - val_loss: 0.1932 - val_accuracy: 0.9603\n",
            "Epoch 74/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1231 - accuracy: 0.9681 - val_loss: 0.1888 - val_accuracy: 0.9632\n",
            "Epoch 75/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9775 - val_loss: 0.1887 - val_accuracy: 0.9603\n",
            "Epoch 76/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9732 - val_loss: 0.1871 - val_accuracy: 0.9632\n",
            "Epoch 77/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.9681 - val_loss: 0.1875 - val_accuracy: 0.9632\n",
            "Epoch 78/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1157 - accuracy: 0.9632 - val_loss: 0.1891 - val_accuracy: 0.9603\n",
            "Epoch 79/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.9670 - val_loss: 0.1882 - val_accuracy: 0.9603\n",
            "Epoch 80/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1066 - accuracy: 0.9632 - val_loss: 0.1973 - val_accuracy: 0.9603\n",
            "Epoch 81/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1135 - accuracy: 0.9652 - val_loss: 0.1874 - val_accuracy: 0.9632\n",
            "Epoch 82/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0995 - accuracy: 0.9743 - val_loss: 0.1863 - val_accuracy: 0.9603\n",
            "Epoch 83/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0990 - accuracy: 0.9753 - val_loss: 0.1870 - val_accuracy: 0.9632\n",
            "Epoch 84/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.9667 - val_loss: 0.1857 - val_accuracy: 0.9632\n",
            "Epoch 85/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1133 - accuracy: 0.9681 - val_loss: 0.1874 - val_accuracy: 0.9632\n",
            "Epoch 86/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9661 - val_loss: 0.1864 - val_accuracy: 0.9632\n",
            "Epoch 87/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1087 - accuracy: 0.9691 - val_loss: 0.1875 - val_accuracy: 0.9632\n",
            "Epoch 88/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1186 - accuracy: 0.9650 - val_loss: 0.1900 - val_accuracy: 0.9603\n",
            "Epoch 89/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9647 - val_loss: 0.1943 - val_accuracy: 0.9632\n",
            "Epoch 90/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1028 - accuracy: 0.9700 - val_loss: 0.1880 - val_accuracy: 0.9603\n",
            "Epoch 91/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9648 - val_loss: 0.1970 - val_accuracy: 0.9632\n",
            "Epoch 92/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.9753 - val_loss: 0.1861 - val_accuracy: 0.9603\n",
            "Epoch 93/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9691 - val_loss: 0.1868 - val_accuracy: 0.9632\n",
            "Epoch 94/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1078 - accuracy: 0.9696 - val_loss: 0.1856 - val_accuracy: 0.9632\n",
            "Epoch 95/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 0.9664 - val_loss: 0.1853 - val_accuracy: 0.9603\n",
            "Epoch 96/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1088 - accuracy: 0.9693 - val_loss: 0.1870 - val_accuracy: 0.9603\n",
            "Epoch 97/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.9692 - val_loss: 0.1913 - val_accuracy: 0.9603\n",
            "Epoch 98/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.9714 - val_loss: 0.1922 - val_accuracy: 0.9632\n",
            "Epoch 99/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1021 - accuracy: 0.9699 - val_loss: 0.1910 - val_accuracy: 0.9632\n",
            "Epoch 100/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1143 - accuracy: 0.9663 - val_loss: 0.1899 - val_accuracy: 0.9632\n",
            "Epoch 101/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9715 - val_loss: 0.1894 - val_accuracy: 0.9603\n",
            "Epoch 102/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1035 - accuracy: 0.9694 - val_loss: 0.1876 - val_accuracy: 0.9603\n",
            "Epoch 103/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1207 - accuracy: 0.9591 - val_loss: 0.2004 - val_accuracy: 0.9632\n",
            "Epoch 104/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9715 - val_loss: 0.1938 - val_accuracy: 0.9632\n",
            "Epoch 105/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9705 - val_loss: 0.2048 - val_accuracy: 0.9603\n",
            "Epoch 106/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1287 - accuracy: 0.9565 - val_loss: 0.1902 - val_accuracy: 0.9632\n",
            "Epoch 107/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1184 - accuracy: 0.9645 - val_loss: 0.1906 - val_accuracy: 0.9603\n",
            "Epoch 108/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9753 - val_loss: 0.1901 - val_accuracy: 0.9632\n",
            "Epoch 109/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0962 - accuracy: 0.9730 - val_loss: 0.1909 - val_accuracy: 0.9603\n",
            "Epoch 110/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1004 - accuracy: 0.9716 - val_loss: 0.1917 - val_accuracy: 0.9603\n",
            "Epoch 111/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1015 - accuracy: 0.9695 - val_loss: 0.1907 - val_accuracy: 0.9603\n",
            "Epoch 112/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.9771 - val_loss: 0.1943 - val_accuracy: 0.9632\n",
            "Epoch 113/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1060 - accuracy: 0.9664 - val_loss: 0.1986 - val_accuracy: 0.9632\n",
            "Epoch 114/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.9747 - val_loss: 0.1944 - val_accuracy: 0.9632\n",
            "Epoch 115/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.9639 - val_loss: 0.1964 - val_accuracy: 0.9603\n",
            "Epoch 116/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9679 - val_loss: 0.1983 - val_accuracy: 0.9632\n",
            "Epoch 117/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0902 - accuracy: 0.9736 - val_loss: 0.1955 - val_accuracy: 0.9603\n",
            "Epoch 118/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9785 - val_loss: 0.1973 - val_accuracy: 0.9603\n",
            "Epoch 119/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9720 - val_loss: 0.1987 - val_accuracy: 0.9603\n",
            "Epoch 120/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.9703 - val_loss: 0.2017 - val_accuracy: 0.9603\n",
            "Epoch 121/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.9678 - val_loss: 0.2030 - val_accuracy: 0.9603\n",
            "Epoch 122/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1088 - accuracy: 0.9663 - val_loss: 0.2059 - val_accuracy: 0.9603\n",
            "Epoch 123/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0949 - accuracy: 0.9750 - val_loss: 0.2030 - val_accuracy: 0.9603\n",
            "Epoch 124/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9698 - val_loss: 0.2033 - val_accuracy: 0.9660\n",
            "Epoch 125/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1266 - accuracy: 0.9608 - val_loss: 0.2037 - val_accuracy: 0.9603\n",
            "Epoch 126/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1125 - accuracy: 0.9638 - val_loss: 0.2048 - val_accuracy: 0.9632\n",
            "Epoch 127/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0983 - accuracy: 0.9721 - val_loss: 0.2070 - val_accuracy: 0.9603\n",
            "Epoch 128/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9752 - val_loss: 0.2064 - val_accuracy: 0.9603\n",
            "Epoch 129/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1030 - accuracy: 0.9680 - val_loss: 0.2120 - val_accuracy: 0.9603\n",
            "Epoch 130/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9769 - val_loss: 0.2100 - val_accuracy: 0.9632\n",
            "Epoch 131/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0964 - accuracy: 0.9708 - val_loss: 0.2125 - val_accuracy: 0.9603\n",
            "Epoch 132/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0959 - accuracy: 0.9729 - val_loss: 0.2185 - val_accuracy: 0.9632\n",
            "Epoch 133/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9745 - val_loss: 0.2201 - val_accuracy: 0.9632\n",
            "Epoch 134/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9692 - val_loss: 0.2206 - val_accuracy: 0.9603\n",
            "Epoch 135/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9648 - val_loss: 0.2206 - val_accuracy: 0.9632\n",
            "Epoch 136/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.9718 - val_loss: 0.2206 - val_accuracy: 0.9603\n",
            "Epoch 137/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.9702 - val_loss: 0.2343 - val_accuracy: 0.9632\n",
            "Epoch 138/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9714 - val_loss: 0.2235 - val_accuracy: 0.9603\n",
            "Epoch 139/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.9693 - val_loss: 0.2279 - val_accuracy: 0.9603\n",
            "Epoch 140/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1107 - accuracy: 0.9667 - val_loss: 0.2281 - val_accuracy: 0.9603\n",
            "Epoch 141/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1050 - accuracy: 0.9679 - val_loss: 0.2294 - val_accuracy: 0.9603\n",
            "Epoch 142/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1126 - accuracy: 0.9674 - val_loss: 0.2383 - val_accuracy: 0.9632\n",
            "Epoch 143/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9691 - val_loss: 0.2403 - val_accuracy: 0.9632\n",
            "Epoch 144/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1123 - accuracy: 0.9640 - val_loss: 0.2362 - val_accuracy: 0.9603\n",
            "Epoch 145/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9743 - val_loss: 0.2393 - val_accuracy: 0.9603\n",
            "Epoch 146/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9754 - val_loss: 0.2414 - val_accuracy: 0.9603\n",
            "Epoch 147/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1012 - accuracy: 0.9708 - val_loss: 0.2418 - val_accuracy: 0.9603\n",
            "Epoch 148/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9729 - val_loss: 0.2451 - val_accuracy: 0.9603\n",
            "Epoch 149/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9744 - val_loss: 0.2496 - val_accuracy: 0.9603\n",
            "Epoch 150/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9723 - val_loss: 0.2498 - val_accuracy: 0.9632\n",
            "Epoch 151/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9771 - val_loss: 0.2554 - val_accuracy: 0.9603\n",
            "Epoch 152/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9748 - val_loss: 0.2604 - val_accuracy: 0.9632\n",
            "Epoch 153/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.9719 - val_loss: 0.2644 - val_accuracy: 0.9603\n",
            "Epoch 154/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1037 - accuracy: 0.9684 - val_loss: 0.2640 - val_accuracy: 0.9603\n",
            "Epoch 155/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9686 - val_loss: 0.2671 - val_accuracy: 0.9603\n",
            "Epoch 156/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.9684 - val_loss: 0.2777 - val_accuracy: 0.9632\n",
            "Epoch 157/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1062 - accuracy: 0.9713 - val_loss: 0.2760 - val_accuracy: 0.9632\n",
            "Epoch 158/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9675 - val_loss: 0.2799 - val_accuracy: 0.9603\n",
            "Epoch 159/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.9725 - val_loss: 0.2756 - val_accuracy: 0.9603\n",
            "Epoch 160/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9755 - val_loss: 0.2800 - val_accuracy: 0.9603\n",
            "Epoch 161/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0878 - accuracy: 0.9737 - val_loss: 0.2858 - val_accuracy: 0.9603\n",
            "Epoch 162/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9680 - val_loss: 0.2961 - val_accuracy: 0.9603\n",
            "Epoch 163/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.9673 - val_loss: 0.2883 - val_accuracy: 0.9660\n",
            "Epoch 164/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1061 - accuracy: 0.9633 - val_loss: 0.2924 - val_accuracy: 0.9632\n",
            "Epoch 165/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1103 - accuracy: 0.9666 - val_loss: 0.3012 - val_accuracy: 0.9603\n",
            "Epoch 166/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9766 - val_loss: 0.3053 - val_accuracy: 0.9632\n",
            "Epoch 167/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1032 - accuracy: 0.9644 - val_loss: 0.3034 - val_accuracy: 0.9603\n",
            "Epoch 168/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 0.9679 - val_loss: 0.3151 - val_accuracy: 0.9632\n",
            "Epoch 169/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9756 - val_loss: 0.3134 - val_accuracy: 0.9632\n",
            "Epoch 170/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0966 - accuracy: 0.9711 - val_loss: 0.3156 - val_accuracy: 0.9632\n",
            "Epoch 171/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1183 - accuracy: 0.9643 - val_loss: 0.3229 - val_accuracy: 0.9632\n",
            "Epoch 172/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 0.9683 - val_loss: 0.3164 - val_accuracy: 0.9603\n",
            "Epoch 173/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9739 - val_loss: 0.3199 - val_accuracy: 0.9632\n",
            "Epoch 174/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1184 - accuracy: 0.9666 - val_loss: 0.3212 - val_accuracy: 0.9632\n",
            "Epoch 175/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9722 - val_loss: 0.3260 - val_accuracy: 0.9603\n",
            "Epoch 176/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9818 - val_loss: 0.3286 - val_accuracy: 0.9632\n",
            "Epoch 177/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0986 - accuracy: 0.9677 - val_loss: 0.3404 - val_accuracy: 0.9603\n",
            "Epoch 178/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9748 - val_loss: 0.3293 - val_accuracy: 0.9603\n",
            "Epoch 179/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9654 - val_loss: 0.3364 - val_accuracy: 0.9632\n",
            "Epoch 180/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0957 - accuracy: 0.9724 - val_loss: 0.3379 - val_accuracy: 0.9632\n",
            "Epoch 181/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.9707 - val_loss: 0.3364 - val_accuracy: 0.9603\n",
            "Epoch 182/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9783 - val_loss: 0.3408 - val_accuracy: 0.9603\n",
            "Epoch 183/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0966 - accuracy: 0.9724 - val_loss: 0.3424 - val_accuracy: 0.9603\n",
            "Epoch 184/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9737 - val_loss: 0.3485 - val_accuracy: 0.9603\n",
            "Epoch 185/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9773 - val_loss: 0.3541 - val_accuracy: 0.9632\n",
            "Epoch 186/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.9765 - val_loss: 0.3510 - val_accuracy: 0.9660\n",
            "Epoch 187/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9732 - val_loss: 0.3605 - val_accuracy: 0.9632\n",
            "Epoch 188/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.9781 - val_loss: 0.3588 - val_accuracy: 0.9603\n",
            "Epoch 189/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.9722 - val_loss: 0.3631 - val_accuracy: 0.9603\n",
            "Epoch 190/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9754 - val_loss: 0.3638 - val_accuracy: 0.9603\n",
            "Epoch 191/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9742 - val_loss: 0.3680 - val_accuracy: 0.9603\n",
            "Epoch 192/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9696 - val_loss: 0.3734 - val_accuracy: 0.9603\n",
            "Epoch 193/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9683 - val_loss: 0.3759 - val_accuracy: 0.9603\n",
            "Epoch 194/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1005 - accuracy: 0.9723 - val_loss: 0.3805 - val_accuracy: 0.9632\n",
            "Epoch 195/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.9762 - val_loss: 0.3847 - val_accuracy: 0.9603\n",
            "Epoch 196/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.9707 - val_loss: 0.3831 - val_accuracy: 0.9603\n",
            "Epoch 197/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.9750 - val_loss: 0.3850 - val_accuracy: 0.9632\n",
            "Epoch 198/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0961 - accuracy: 0.9677 - val_loss: 0.3884 - val_accuracy: 0.9603\n",
            "Epoch 199/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9692 - val_loss: 0.3882 - val_accuracy: 0.9603\n",
            "Epoch 200/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9680 - val_loss: 0.3949 - val_accuracy: 0.9632\n",
            "Epoch 201/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 0.9782 - val_loss: 0.3957 - val_accuracy: 0.9632\n",
            "Epoch 202/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0898 - accuracy: 0.9747 - val_loss: 0.3975 - val_accuracy: 0.9603\n",
            "Epoch 203/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0907 - accuracy: 0.9717 - val_loss: 0.4082 - val_accuracy: 0.9632\n",
            "Epoch 204/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0892 - accuracy: 0.9724 - val_loss: 0.4039 - val_accuracy: 0.9632\n",
            "Epoch 205/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0962 - accuracy: 0.9710 - val_loss: 0.4119 - val_accuracy: 0.9603\n",
            "Epoch 206/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1093 - accuracy: 0.9657 - val_loss: 0.4171 - val_accuracy: 0.9603\n",
            "Epoch 207/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9722 - val_loss: 0.4111 - val_accuracy: 0.9603\n",
            "Epoch 208/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 0.9748 - val_loss: 0.4183 - val_accuracy: 0.9632\n",
            "Epoch 209/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.9749 - val_loss: 0.4157 - val_accuracy: 0.9603\n",
            "Epoch 210/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0918 - accuracy: 0.9702 - val_loss: 0.4252 - val_accuracy: 0.9603\n",
            "Epoch 211/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9791 - val_loss: 0.4185 - val_accuracy: 0.9603\n",
            "Epoch 212/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9712 - val_loss: 0.4269 - val_accuracy: 0.9603\n",
            "Epoch 213/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9707 - val_loss: 0.4279 - val_accuracy: 0.9603\n",
            "Epoch 214/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.9717 - val_loss: 0.4339 - val_accuracy: 0.9632\n",
            "Epoch 215/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1188 - accuracy: 0.9602 - val_loss: 0.4355 - val_accuracy: 0.9632\n",
            "Epoch 216/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1182 - accuracy: 0.9613 - val_loss: 0.4398 - val_accuracy: 0.9603\n",
            "Epoch 217/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.9695 - val_loss: 0.4432 - val_accuracy: 0.9603\n",
            "Epoch 218/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0998 - accuracy: 0.9679 - val_loss: 0.4501 - val_accuracy: 0.9632\n",
            "Epoch 219/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9723 - val_loss: 0.4520 - val_accuracy: 0.9603\n",
            "Epoch 220/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0967 - accuracy: 0.9708 - val_loss: 0.4582 - val_accuracy: 0.9632\n",
            "Epoch 221/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9774 - val_loss: 0.4554 - val_accuracy: 0.9603\n",
            "Epoch 222/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9706 - val_loss: 0.4680 - val_accuracy: 0.9603\n",
            "Epoch 223/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9711 - val_loss: 0.4765 - val_accuracy: 0.9603\n",
            "Epoch 224/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9726 - val_loss: 0.4781 - val_accuracy: 0.9632\n",
            "Epoch 225/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9741 - val_loss: 0.4773 - val_accuracy: 0.9660\n",
            "Epoch 226/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.9692 - val_loss: 0.4801 - val_accuracy: 0.9632\n",
            "Epoch 227/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 0.9683 - val_loss: 0.4908 - val_accuracy: 0.9603\n",
            "Epoch 228/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9701 - val_loss: 0.4890 - val_accuracy: 0.9603\n",
            "Epoch 229/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0871 - accuracy: 0.9725 - val_loss: 0.4954 - val_accuracy: 0.9603\n",
            "Epoch 230/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9670 - val_loss: 0.4991 - val_accuracy: 0.9632\n",
            "Epoch 231/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.9742 - val_loss: 0.4947 - val_accuracy: 0.9603\n",
            "Epoch 232/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.9728 - val_loss: 0.5039 - val_accuracy: 0.9632\n",
            "Epoch 233/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0925 - accuracy: 0.9693 - val_loss: 0.5154 - val_accuracy: 0.9603\n",
            "Epoch 234/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.9686 - val_loss: 0.5018 - val_accuracy: 0.9603\n",
            "Epoch 235/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9778 - val_loss: 0.5010 - val_accuracy: 0.9603\n",
            "Epoch 236/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.9730 - val_loss: 0.5026 - val_accuracy: 0.9603\n",
            "Epoch 237/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1008 - accuracy: 0.9665 - val_loss: 0.5148 - val_accuracy: 0.9632\n",
            "Epoch 238/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9799 - val_loss: 0.5100 - val_accuracy: 0.9603\n",
            "Epoch 239/300\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9760 - val_loss: 0.5111 - val_accuracy: 0.9603\n",
            "Epoch 240/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0960 - accuracy: 0.9680 - val_loss: 0.5119 - val_accuracy: 0.9603\n",
            "Epoch 241/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9701 - val_loss: 0.5141 - val_accuracy: 0.9603\n",
            "Epoch 242/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.9758 - val_loss: 0.5177 - val_accuracy: 0.9660\n",
            "Epoch 243/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0892 - accuracy: 0.9694 - val_loss: 0.5335 - val_accuracy: 0.9632\n",
            "Epoch 244/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1012 - accuracy: 0.9667 - val_loss: 0.5224 - val_accuracy: 0.9603\n",
            "Epoch 245/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.9703 - val_loss: 0.5273 - val_accuracy: 0.9603\n",
            "Epoch 246/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9766 - val_loss: 0.5275 - val_accuracy: 0.9603\n",
            "Epoch 247/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.9704 - val_loss: 0.5267 - val_accuracy: 0.9603\n",
            "Epoch 248/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0871 - accuracy: 0.9734 - val_loss: 0.5292 - val_accuracy: 0.9603\n",
            "Epoch 249/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.9702 - val_loss: 0.5354 - val_accuracy: 0.9632\n",
            "Epoch 250/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.9697 - val_loss: 0.5348 - val_accuracy: 0.9603\n",
            "Epoch 251/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9756 - val_loss: 0.5364 - val_accuracy: 0.9603\n",
            "Epoch 252/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9710 - val_loss: 0.5426 - val_accuracy: 0.9603\n",
            "Epoch 253/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1015 - accuracy: 0.9699 - val_loss: 0.5443 - val_accuracy: 0.9660\n",
            "Epoch 254/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.9720 - val_loss: 0.5467 - val_accuracy: 0.9603\n",
            "Epoch 255/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.9691 - val_loss: 0.5564 - val_accuracy: 0.9603\n",
            "Epoch 256/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.9674 - val_loss: 0.5598 - val_accuracy: 0.9603\n",
            "Epoch 257/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.9677 - val_loss: 0.5520 - val_accuracy: 0.9603\n",
            "Epoch 258/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.9781 - val_loss: 0.5496 - val_accuracy: 0.9603\n",
            "Epoch 259/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.9755 - val_loss: 0.5515 - val_accuracy: 0.9603\n",
            "Epoch 260/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9711 - val_loss: 0.5539 - val_accuracy: 0.9603\n",
            "Epoch 261/300\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9747 - val_loss: 0.5555 - val_accuracy: 0.9603\n",
            "Epoch 262/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.9737 - val_loss: 0.5597 - val_accuracy: 0.9603\n",
            "Epoch 263/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.9737 - val_loss: 0.5635 - val_accuracy: 0.9603\n",
            "Epoch 264/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9664 - val_loss: 0.5715 - val_accuracy: 0.9603\n",
            "Epoch 265/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9756 - val_loss: 0.5730 - val_accuracy: 0.9603\n",
            "Epoch 266/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.9671 - val_loss: 0.5792 - val_accuracy: 0.9603\n",
            "Epoch 267/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.9748 - val_loss: 0.5755 - val_accuracy: 0.9603\n",
            "Epoch 268/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9713 - val_loss: 0.5776 - val_accuracy: 0.9603\n",
            "Epoch 269/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1051 - accuracy: 0.9661 - val_loss: 0.5847 - val_accuracy: 0.9603\n",
            "Epoch 270/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9808 - val_loss: 0.5835 - val_accuracy: 0.9603\n",
            "Epoch 271/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9705 - val_loss: 0.5900 - val_accuracy: 0.9603\n",
            "Epoch 272/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9686 - val_loss: 0.5911 - val_accuracy: 0.9603\n",
            "Epoch 273/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9807 - val_loss: 0.5900 - val_accuracy: 0.9660\n",
            "Epoch 274/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9760 - val_loss: 0.6012 - val_accuracy: 0.9603\n",
            "Epoch 275/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9764 - val_loss: 0.5953 - val_accuracy: 0.9632\n",
            "Epoch 276/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.9683 - val_loss: 0.6088 - val_accuracy: 0.9632\n",
            "Epoch 277/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.9789 - val_loss: 0.6023 - val_accuracy: 0.9632\n",
            "Epoch 278/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9702 - val_loss: 0.6117 - val_accuracy: 0.9603\n",
            "Epoch 279/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0940 - accuracy: 0.9712 - val_loss: 0.6105 - val_accuracy: 0.9603\n",
            "Epoch 280/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.9675 - val_loss: 0.6142 - val_accuracy: 0.9603\n",
            "Epoch 281/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.9673 - val_loss: 0.6143 - val_accuracy: 0.9632\n",
            "Epoch 282/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0925 - accuracy: 0.9727 - val_loss: 0.6161 - val_accuracy: 0.9603\n",
            "Epoch 283/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.1008 - accuracy: 0.9658 - val_loss: 0.6259 - val_accuracy: 0.9603\n",
            "Epoch 284/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9729 - val_loss: 0.6224 - val_accuracy: 0.9603\n",
            "Epoch 285/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9707 - val_loss: 0.6305 - val_accuracy: 0.9603\n",
            "Epoch 286/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0936 - accuracy: 0.9727 - val_loss: 0.6301 - val_accuracy: 0.9603\n",
            "Epoch 287/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0845 - accuracy: 0.9708 - val_loss: 0.6404 - val_accuracy: 0.9632\n",
            "Epoch 288/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9778 - val_loss: 0.6347 - val_accuracy: 0.9688\n",
            "Epoch 289/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9770 - val_loss: 0.6317 - val_accuracy: 0.9660\n",
            "Epoch 290/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0956 - accuracy: 0.9661 - val_loss: 0.6349 - val_accuracy: 0.9603\n",
            "Epoch 291/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0856 - accuracy: 0.9743 - val_loss: 0.6438 - val_accuracy: 0.9603\n",
            "Epoch 292/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9762 - val_loss: 0.6411 - val_accuracy: 0.9660\n",
            "Epoch 293/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.9767 - val_loss: 0.6598 - val_accuracy: 0.9603\n",
            "Epoch 294/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9723 - val_loss: 0.6465 - val_accuracy: 0.9603\n",
            "Epoch 295/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9706 - val_loss: 0.6560 - val_accuracy: 0.9632\n",
            "Epoch 296/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9658 - val_loss: 0.6677 - val_accuracy: 0.9632\n",
            "Epoch 297/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9740 - val_loss: 0.6551 - val_accuracy: 0.9632\n",
            "Epoch 298/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9611 - val_loss: 0.6599 - val_accuracy: 0.9660\n",
            "Epoch 299/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9683 - val_loss: 0.6615 - val_accuracy: 0.9603\n",
            "Epoch 300/300\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9777 - val_loss: 0.6647 - val_accuracy: 0.9603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2_Abe8LvBRg"
      },
      "source": [
        "# Part 3 - Making the predictions and evaluating the model\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcLkZ1tQvM8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "466d6ed8-d113-4787-d227-afceb67a5283"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "y_test_class = np.argmax(y_test, axis=1)\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_test_class, y_pred_class))\n",
        "print(confusion_matrix(y_test_class, y_pred_class))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.91      0.93       132\n",
            "           1       0.96      0.98      0.97       309\n",
            "\n",
            "    accuracy                           0.96       441\n",
            "   macro avg       0.96      0.95      0.95       441\n",
            "weighted avg       0.96      0.96      0.96       441\n",
            "\n",
            "[[120  12]\n",
            " [  5 304]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2c2dykavQpc"
      },
      "source": [
        "# Calculate the Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "score=accuracy_score(y_pred,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzbx0KfD7A4g"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIfI8YE8vVQ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "ece8bdb4-83ba-4b35-e656-c27ccdb84968"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "from matplotlib import pyplot\n",
        "# perform permutation importance\n",
        "results = permutation_importance(classifier, X_train, y_train, scoring='neg_mean_squared_error')\n",
        "# get importance\n",
        "importance = results.importances_mean\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
        "# plot feature importance\n",
        "pyplot.bar([x for x in range(len(importance))], importance)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature: 0, Score: 0.13684\n",
            "Feature: 1, Score: 0.10807\n",
            "Feature: 2, Score: 0.12918\n",
            "Feature: 3, Score: 0.16154\n",
            "Feature: 4, Score: 0.03564\n",
            "Feature: 5, Score: 0.14529\n",
            "Feature: 6, Score: 0.21902\n",
            "Feature: 7, Score: 0.11317\n",
            "Feature: 8, Score: 0.16819\n",
            "Feature: 9, Score: 0.17719\n",
            "Feature: 10, Score: 0.30061\n",
            "Feature: 11, Score: 0.08679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD7CAYAAACCEpQdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYC0lEQVR4nO3da2yT593H8V/ssBIKEXGWgymnglTmlbBqRSDUwjQIONucOkJirjL6BhrWwZbCuops0nKAoi1IQwVKujVqmRDahiKmZJiMZqxMEKb1ICEKNe0QSwYlTsLsRhxU2tXx86KqnydPAN9JnDj29f288uG6ff3/TvKLc9n3lYxoNBoVAMAotmQXAAAYe4Q/ABiI8AcAAxH+AGAgwh8ADET4A4CBCH8AMFBmsguw6qOPbqm/f3RPScjNnaxQ6OaozjFW0qkXKb36SadeJPoZr2y2DOXk3H/X+1Mm/Pv7o6Me/l/Mky7SqRcpvfpJp14k+klFLPsAgIEIfwAwEOEPAAYi/AHAQJbCv6OjQz6fT263Wz6fT52dnYPGHD58WKWlpfJ6vSotLdWBAwdi90UiEdXV1am4uFgrV65UU1NTwhoAAAydpU/71NTUqLy8XF6vVy0tLaqurh4Q7pLkdru1evVqZWRk6ObNmyotLdWiRYv0la98RUeOHNHly5fV1tamvr4+lZWVacmSJZo+ffqoNAUAuLe4r/xDoZACgYA8Ho8kyePxKBAIKBwODxg3efJkZWRkSJJu376t//73v7Hrra2tWrNmjWw2mxwOh4qLi3Xs2LFE9wIAsCjuK/9gMKiCggLZ7XZJkt1uV35+voLBoBwOx4Cxf/3rX7Vr1y5dvnxZzz33nObNmxd7jGnTpsXGOZ1OdXd3J7IPALirKdlZmnif9dOa8vKmxB1z+5PPdOP6xyMpK6kSepLXihUrtGLFCnV1dWnTpk1atmyZ5syZk5DHzs2dnJDHicfKFz1VpFMvUnr1k069SKnRT+lzLQl9vCO/8mpiCvR9N3HD3+l0qqenR5FIRHa7XZFIRL29vXI6nXc9Ztq0aSoqKtLf/vY3zZkzR06nU11dXVqwYIGkwX8JWBEK3Rz1s+7y8qbo2rUbozrHWEmnXqT06iedepFSo5/R+uU0nvu22TLu+aI57pp/bm6uXC6X/H6/JMnv98vlcg1a8rl06VLscjgc1ptvvqmHHnpIklRSUqKmpib19/crHA7r+PHjcrvdw2oIADBylpZ9amtrVVVVpYaGBmVnZ6u+vl6SVFFRocrKShUVFenQoUM6ffq0MjMzFY1GtXbtWj3++OOSJK/Xq7Nnz2rVqlWSpE2bNmnGjBmj1BIAIJ6MaDSaEjsYsewzNOnUi5Re/aRTL1Jq9JOXN2VU1vzHc98jXvYBAKQfwh8ADET4A4CBCH8AMBDhDwAGIvwBwECEPwAYiPAHAAMR/gBgIMIfAAxE+AOAgQh/ADAQ4Q8ABiL8AcBAhD8AGIjwBwADEf4AYCDCHwAMRPgDgIEIfwAwEOEPAAYi/AHAQIQ/ABiI8AcAAxH+AGCgTCuDOjo6VFVVpb6+Pk2dOlX19fWaPXv2gDH79u1Ta2urbDabJkyYoC1btmjp0qWSpKqqKv39739XTk6OJKmkpEQ/+MEPEtsJAMAyS+FfU1Oj8vJyeb1etbS0qLq6WgcOHBgwZsGCBVq3bp2ysrL0/vvva+3atWpvb9fEiRMlSRs2bNDatWsT3wEAYMjiLvuEQiEFAgF5PB5JksfjUSAQUDgcHjBu6dKlysrKkiTNmzdP0WhUfX19o1AyAGCk4oZ/MBhUQUGB7Ha7JMlutys/P1/BYPCuxzQ3N2vmzJkqLCyM3bZ//36VlpZq48aNunTpUgJKBwAMl6Vln6F46623tHv3br322mux27Zs2aK8vDzZbDY1Nzfr6aef1vHjx2O/UKzIzZ2c6FLvKC9vypjMMxbSqRcpvfpJp16k9OvHqlTuO274O51O9fT0KBKJyG63KxKJqLe3V06nc9DYM2fO6Pnnn1dDQ4PmzJkTu72goCB2uaysTL/4xS/U3d2tBx54wHKhodBN9fdHLY8fjry8Kbp27caozjFW0qkXKb36SadepNToZ7RCejz3bbNl3PNFc9xln9zcXLlcLvn9fkmS3++Xy+WSw+EYMO7dd9/Vli1btGfPHj388MMD7uvp6YldPnXqlGw224BfCACAsWVp2ae2tlZVVVVqaGhQdna26uvrJUkVFRWqrKxUUVGR6urqdPv2bVVXV8eO27lzp+bNm6etW7cqFAopIyNDkydP1ssvv6zMzISvOAEALLKUwHPnzlVTU9Og2xsbG2OXDx8+fNfjf/vb3w69MgDAqOEMXwAwEOEPAAYi/AHAQIQ/ABiI8AcAAxH+AGAgwh8ADET4A4CBCH8AMBDhDwAGIvwBwECEPwAYiPAHAAMR/gBgIMIfAAxE+AOAgfh3WkCCTMnO0sT7rP1IWf2fsrc/+Uw3rn88krKAOyL8gQSZeF+mSp9rSehjHvmVV+P3X4QjlbHsAwAGIvwBwECEPwAYiPAHAAMR/gBgIMIfAAxE+AOAgQh/ADCQpfDv6OiQz+eT2+2Wz+dTZ2fnoDH79u3Td77zHZWWlmr16tU6depU7L6PP/5Ymzdv1sqVK1VSUqITJ04krAEAwNBZOsO3pqZG5eXl8nq9amlpUXV1tQ4cODBgzIIFC7Ru3TplZWXp/fff19q1a9Xe3q6JEyfq1Vdf1eTJk/WXv/xFnZ2d+t73vqe2tjbdf//9o9IUAODe4r7yD4VCCgQC8ng8kiSPx6NAIKBwODxg3NKlS5WVlSVJmjdvnqLRqPr6+iRJf/7zn+Xz+SRJs2fP1vz583Xy5MmENgIAsC5u+AeDQRUUFMhut0uS7Ha78vPzFQwG73pMc3OzZs6cqcLCQklSV1eXHnjggdj9TqdT3d3dI60dADBMCd/Y7a233tLu3bv12muvJfRxc3MnJ/Tx7sbqboupIJ16kdKvH6tSoe9UqHE0pHLfccPf6XSqp6dHkUhEdrtdkUhEvb29cjqdg8aeOXNGzz//vBoaGjRnzpzY7dOmTdPVq1flcDgkff7XxOLFi4dUaCh0U/390SEdM1R5eVN07Vp67KGYTr1IqdHPaAVBKvSdCjWOhvHct82Wcc8XzXGXfXJzc+VyueT3+yVJfr9fLpcrFuRfePfdd7Vlyxbt2bNHDz/88ID7SkpKdOjQIUlSZ2enzp07p6VLlw65GQBAYlj6qGdtba0OHjwot9utgwcPqq6uTpJUUVGhc+fOSZLq6up0+/ZtVVdXy+v1yuv16oMPPpAkrV+/XtevX9fKlSv1/e9/X9u2bdPkyWOzjAMAGMzSmv/cuXPV1NQ06PbGxsbY5cOHD9/1+EmTJmnPnj3DKA8AMBo4wxcADET4A4CBCH8AMBDhDwAGIvwBwECEPwAYiPAHAAMR/gBgIMIfAAxE+AOAgRK+pTMAWDUlO0sT70tsDN3+5DPduP5xQh8zHRH+AJJm4n2ZKn2uJaGPeeRXXo3fjZbHD5Z9AMBAhD8AGIjwBwADEf4AYCDe8AVwR0P5JI6V/5HLp3DGF8IfwB0l+pM4fApnfGHZBwAMRPgDgIEIfwAwEGv+uKNEn3bPm33A+EL44454sw9Ibyz7AICBCH8AMJCl8O/o6JDP55Pb7ZbP51NnZ+egMe3t7Vq9erXmz5+v+vr6Afft3btXS5YskdfrldfrVV1dXUKKBwAMj6U1/5qaGpWXl8vr9aqlpUXV1dU6cODAgDEzZszQjh07dOzYMX366aeDHqOsrExbt25NTNUAgBGJ+8o/FAopEAjI4/FIkjwejwKBgMLh8IBxs2bNksvlUmYm7yEDwHgXN/yDwaAKCgpkt9slSXa7Xfn5+QoGg0Oa6OjRoyotLdW6det05syZ4VULAEiIMXmZ/uSTT+qZZ57RhAkTdPr0aW3cuFGtra3Kycmx/Bi5uZNHscL/ZWWDqlQx3noZaT3jrZ+xkk59j1Uv6TbPaIgb/k6nUz09PYpEIrLb7YpEIurt7ZXT6bQ8SV5eXuzyY489JqfTqYsXL2rRokWWHyMUuqn+/qjl8cORlzdF166lx6fRR9rLaHxTj7Se8f61Ga0gSFbfY/E9MFbPWbp9bayw2TLu+aI57rJPbm6uXC6X/H6/JMnv98vlcsnhcFguoqenJ3b5woULunr1qh588EHLxwMAEsvSsk9tba2qqqrU0NCg7Ozs2Ec5KyoqVFlZqaKiIr3zzjv68Y9/rJs3byoajero0aPasWOHli5dql27dum9996TzWbThAkTtHPnzgF/DQCjKdFbVUhsV4HUZ+knYu7cuWpqahp0e2NjY+zywoULdfLkyTse//8/9w+MpURvVSGxXQVSH2f4AoCBCH8AMBDhDwAGIvwBwEBpvxfDUD/pYeXzwHzSA0CqS/vw55MeADAYyz4AYCDCHwAMRPgDgIHSfs0/3QzlDWyrm1nxBjZgHsI/xfAGNoBEYNkHAAxE+AOAgQh/ADAQ4Q8ABiL8AcBAhD8AGIjwBwADEf4AYCDCHwAMxBm+QIoZ6v+oiIftPcxE+AMpJtFbfLC9h5lY9gEAAxH+AGAgln0SJNHrsBJrsQBGD+GfIGy1DCCVWFr26ejokM/nk9vtls/nU2dn56Ax7e3tWr16tebPn6/6+voB90UiEdXV1am4uFgrV65UU1NTQooHAAyPpfCvqalReXm5Xn/9dZWXl6u6unrQmBkzZmjHjh1av379oPuOHDmiy5cvq62tTYcOHdLevXv14Ycfjrx6AMCwxA3/UCikQCAgj8cjSfJ4PAoEAgqHwwPGzZo1Sy6XS5mZg1eSWltbtWbNGtlsNjkcDhUXF+vYsWMJagEAMFRx1/yDwaAKCgpkt9slSXa7Xfn5+QoGg3I4HJYmCQaDmjZtWuy60+lUd3f3kArNzZ08pPGjzer/x2WexM0xVs+FVXxtmGe8fU8ORcq84RsK3VR/f3TIx43WF+fatYFvxTLP0OcYiry8KcM+PpWfs7Ga507P7VjMk8rP2Z3mGU9stox7vmiOu+zjdDrV09OjSCQi6fM3b3t7e+V0Oi0X4XQ61dXVFbseDAZVWFho+XgAQGLFDf/c3Fy5XC75/X5Jkt/vl8vlsrzkI0klJSVqampSf3+/wuGwjh8/LrfbPfyqAQAjYunTPrW1tTp48KDcbrcOHjyouro6SVJFRYXOnTsnSXrnnXe0bNky7d+/X3/4wx+0bNkynTp1SpLk9Xo1ffp0rVq1St/97ne1adMmzZgxY5RaAgDEY2nNf+7cuXf8bH5jY2Ps8sKFC3Xy5Mk7Hm+322O/MAAAycfePgBgIMIfAAxE+AOAgQh/ADBQypzkBQDjXSpt7U74A0CCpNLW7iz7AICBCH8AMBDhDwAGIvwBwECEPwAYiPAHAAMR/gBgIMIfAAxE+AOAgQh/ADAQ4Q8ABiL8AcBAhD8AGIjwBwADEf4AYCDCHwAMRPgDgIEIfwAwEOEPAAay9D98Ozo6VFVVpb6+Pk2dOlX19fWaPXv2gDGRSEQvvPCCTp06pYyMDG3YsEFr1qyRJO3du1e/+93vlJ+fL0n6+te/rpqamsR2AgCwzFL419TUqLy8XF6vVy0tLaqurtaBAwcGjDly5IguX76strY29fX1qaysTEuWLNH06dMlSWVlZdq6dWviOwAADFncZZ9QKKRAICCPxyNJ8ng8CgQCCofDA8a1trZqzZo1stlscjgcKi4u1rFjx0anagDAiMQN/2AwqIKCAtntdkmS3W5Xfn6+gsHgoHHTpk2LXXc6neru7o5dP3r0qEpLS7Vu3TqdOXMmUfUDAIbB0rLPSD355JN65plnNGHCBJ0+fVobN25Ua2urcnJyLD9Gbu7kUaxw6PLypjDPGM8xVs+FVXxtmCeV54kb/k6nUz09PYpEIrLb7YpEIurt7ZXT6Rw0rqurSwsWLJA08C+BvLy82LjHHntMTqdTFy9e1KJFiywXGgrdVH9/1PL4L4zWF+fatRvMM8I5hiIvb8qwj0/l52ys5rnTczsW86Tyc5bMeayw2TLu+aI57rJPbm6uXC6X/H6/JMnv98vlcsnhcAwYV1JSoqamJvX39yscDuv48eNyu92SpJ6enti4Cxcu6OrVq3rwwQeH3AwAIDEsLfvU1taqqqpKDQ0Nys7OVn19vSSpoqJClZWVKioqktfr1dmzZ7Vq1SpJ0qZNmzRjxgxJ0q5du/Tee+/JZrNpwoQJ2rlz54C/BgAAY8tS+M+dO1dNTU2Dbm9sbIxdttvtqquru+PxX/yyAACMD5zhCwAGIvwBwECEPwAYiPAHAAMR/gBgoDE5wxe4kynZWZp4n/VvQSsn0Nz+5DPduP7xSMoCjED4I2km3pep0udaEvqYR37l1fDPIwbMwbIPABiI8AcAAxH+AGAgwh8ADET4A4CBCH8AMBDhDwAGIvwBwECEPwAYiPAHAAMR/gBgIMIfAAxE+AOAgQh/ADAQ4Q8ABiL8AcBAhD8AGIjwBwADEf4AYCBL4d/R0SGfzye32y2fz6fOzs5BYyKRiOrq6lRcXKyVK1eqqanJ0n0AgLFnKfxrampUXl6u119/XeXl5aqurh405siRI7p8+bLa2tp06NAh7d27Vx9++GHc+wAAYy8z3oBQKKRAIKD9+/dLkjwej7Zv365wOCyHwxEb19raqjVr1shms8nhcKi4uFjHjh3T008/fc/7rLLZMobR3ufyc7KGfezd3Kke5kn+HMyTuDnGap5Ufc6SPc+Ij4nGce7cuei3v/3tAbd961vfip4/f37AbR6PJ3r27NnY9VdeeSW6ffv2uPcBAMYeb/gCgIHihr/T6VRPT48ikYikz9+87e3tldPpHDSuq6srdj0YDKqwsDDufQCAsRc3/HNzc+VyueT3+yVJfr9fLpdrwHq/JJWUlKipqUn9/f0Kh8M6fvy43G533PsAAGMvIxqNRuMNunTpkqqqqnT9+nVlZ2ervr5ec+bMUUVFhSorK1VUVKRIJKJt27bp9OnTkqSKigr5fD5Juud9AICxZyn8AQDphTd8AcBAhD8AGIjwBwADEf4AYCDCX9Y2rksVH330kSoqKuR2u1VaWqof/vCHCofDyS5rxF566SXNmzdP//znP5Ndyoh88sknqqmp0apVq1RaWqqf//znyS5p2E6cOKGysjJ5vV498cQTamtrS3ZJQ1JfX6/ly5cP+r5Kpzy4p+SeYDw+PPXUU9Hm5uZoNBqNNjc3R5966qkkVzR8H330UfQf//hH7Povf/nL6E9/+tMkVjRy58+fj65fvz76zW9+M/rBBx8ku5wR2b59e3THjh3R/v7+aDQajV67di3JFQ1Pf39/dOHChbGvx4ULF6KPPPJINBKJJLky695+++1oV1fXoO+rdMqDezH+lf8XG9d5PB5Jn29cFwgEUvbV8tSpU7V48eLY9UceeWTA2dWp5tNPP9W2bdtUW1ub7FJG7NatW2pubtazzz6rjIzPN9368pe/nOSqhs9ms+nGjRuSpBs3big/P182W+pEysKFCwftVJBueXAvcXf1THfBYFAFBQWy2+2SJLvdrvz8fAWDwUFnMaea/v5+/f73v9fy5cuTXcqw7d69W0888YSmT5+e7FJG7MqVK5o6dapeeuklvfnmm7r//vv17LPPauHChckubcgyMjL04osvauPGjZo0aZJu3bqlV155JdlljVg658H/lzq/pjFk27dv16RJk7R27dpklzIsZ86c0fnz51VeXp7sUhIiEonoypUr+upXv6o//vGP+slPfqIf/ehHunnzZrJLG7LPPvtMv/nNb9TQ0KATJ07o5Zdf1ubNm3Xr1q1klwaLjA9/qxvXpZr6+nr9+9//1osvvphSf4r/X2+//bYuXbqkFStWaPny5eru7tb69evV3t6e7NKGxel0KjMzM7ak8LWvfU05OTnq6OhIcmVDd+HCBfX29urRRx+VJD366KPKysrSpUuXklzZyKRrHtxJaqZCAlnduC6V7Nq1S+fPn9e+ffv0pS99KdnlDNuGDRvU3t6uN954Q2+88YYKCwv16quv6vHHH092acPicDi0ePHi2B5XHR0dCoVCmjVrVpIrG7rCwkJ1d3frX//6l6TP9/8KhUKaOXNmkisbmXTMg7thbx/dfeO6VHTx4kV5PB7Nnj1bEydOlCRNnz5d+/btS3JlI7d8+XL9+te/1kMPPZTsUobtypUr+tnPfqa+vj5lZmZq8+bN+sY3vpHssoblT3/6kxobG2NvXldWVqq4uDjJVVn3wgsvqK2tTf/5z3+Uk5OjqVOn6ujRo2mVB/dC+AOAgYxf9gEAExH+AGAgwh8ADET4A4CBCH8AMBDhDwAGIvwBwECEPwAY6H8A0HEauDIqpDkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF8UpiUe-MqW"
      },
      "source": [
        "classifier.save('stable.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RreHggeNiSiz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4ef1a4a-6f7e-4c67-d195-1349c2aa7a7f"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeClassifier    \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
        "from sklearn import tree\n",
        "\n",
        "clf_gini = DecisionTreeClassifier(criterion = \"gini\",                # Criterion\n",
        "                                  max_depth =4 ,  \n",
        "                                  min_samples_split = 2, # Max Height of Tree\n",
        "                                  min_samples_leaf = 1,              # Maximum Leaf samples\n",
        "                                  random_state = 100)\n",
        "\n",
        "clf_gini.fit(X_train, y_train)\n",
        "y_pred_gini = clf_gini.predict(X_test)  \n",
        "\n",
        "print (\"Accuracy : \", accuracy_score(y_test,y_pred_gini)*100)         # Evaulating predictions with test labels\n",
        "print (\"Report : \",  classification_report(y_test, y_pred_gini))\n",
        "text_representation = tree.export_text(clf_gini, feature_names=X.columns.tolist())\n",
        "print(text_representation)\n",
        "# get importance\n",
        "importance = clf_gini.feature_importances_\n",
        "# summarize feature importance\n",
        "for feature_names,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (feature_names,v))\n",
        "# plot feature importance\n",
        "pyplot.bar([x for x in range(len(importance))], importance, tick_label= X.columns)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  95.91836734693877\n",
            "Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.92      0.93       132\n",
            "           1       0.97      0.97      0.97       309\n",
            "\n",
            "    accuracy                           0.96       441\n",
            "   macro avg       0.95      0.95      0.95       441\n",
            "weighted avg       0.96      0.96      0.96       441\n",
            "\n",
            "|--- FS1 <= 6.58\n",
            "|   |--- SE1 <= 59.65\n",
            "|   |   |--- PS1 <= 173.45\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- PS1 >  173.45\n",
            "|   |   |   |--- SE1 <= 31.26\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- SE1 >  31.26\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |--- SE1 >  59.65\n",
            "|   |   |--- CE1 <= 21.73\n",
            "|   |   |   |--- FS1 <= 6.56\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |   |   |--- FS1 >  6.56\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |--- CE1 >  21.73\n",
            "|   |   |   |--- class: 0\n",
            "|--- FS1 >  6.58\n",
            "|   |--- CE1 <= 27.02\n",
            "|   |   |--- TS1 <= 54.28\n",
            "|   |   |   |--- CE1 <= 26.62\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |   |   |--- CE1 >  26.62\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |   |--- TS1 >  54.28\n",
            "|   |   |   |--- FS1 <= 6.59\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |   |   |--- FS1 >  6.59\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |--- CE1 >  27.02\n",
            "|   |   |--- PS5 <= 9.88\n",
            "|   |   |   |--- CP1 <= 2.18\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- CP1 >  2.18\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |   |--- PS5 >  9.88\n",
            "|   |   |   |--- PS1 <= 161.15\n",
            "|   |   |   |   |--- class: 0\n",
            "|   |   |   |--- PS1 >  161.15\n",
            "|   |   |   |   |--- class: 1\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1866\n",
              "                \n",
              "                    &plusmn; 0.0124\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                CE1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 82.14%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1587\n",
              "                \n",
              "                    &plusmn; 0.0090\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                FS1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 85.25%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1207\n",
              "                \n",
              "                    &plusmn; 0.0065\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                SE1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 88.81%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0814\n",
              "                \n",
              "                    &plusmn; 0.0068\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                PS5\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.12%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0585\n",
              "                \n",
              "                    &plusmn; 0.0068\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                TS1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 93.84%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0347\n",
              "                \n",
              "                    &plusmn; 0.0066\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                PS1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.41%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0101\n",
              "                \n",
              "                    &plusmn; 0.0022\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                CP1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0\n",
              "                \n",
              "                    &plusmn; 0.0000\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                VS1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0\n",
              "                \n",
              "                    &plusmn; 0.0000\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                P1\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0\n",
              "                \n",
              "                    &plusmn; 0.0000\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                FS2\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0\n",
              "                \n",
              "                    &plusmn; 0.0000\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                PS4\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0\n",
              "                \n",
              "                    &plusmn; 0.0000\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                PS3\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH2913VycEGt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}